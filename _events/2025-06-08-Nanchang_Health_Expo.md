---
title: TAS Lab Presents Wearable Navigation Concept for Visually Impaired Individuals at Nanchang Health Expo
subtitle: news
# author:
image: images/news/Nanchang_Health_Expo/ProfWen&Xiangru.jpg
tags: news
order:
---

## TAS Lab Presents Wearable Navigation Concept for Visually Impaired Individuals at Nanchang Health Expo

[cite_start]**NANCHANG – June 6, 2025** – At the Nanchang Health Expo today, the TAS Lab presented an innovative concept for a wearable navigation system designed to enhance independence for visually impaired individuals. [cite_start]The goal is to create a system that offers a deeper, more interactive understanding of the user's surroundings to significantly improve mobility and safety.

![PolyU Stage at Nanchang Health Expo](images/news/Nanchang_Health_Expo/overview.jpg)

The proposed design is built on a sophisticated technology stack. [cite_start]It integrates multi-sensor fusion, using IMU, GNSS, and depth cameras, to achieve precise 3D environmental mapping and pathfinding. [cite_start]An onboard AI vision engine would identify dynamic and static obstacles. [cite_start]Furthermore, it would allow users to find specific objects using simple voice commands. [cite_start]Interaction is envisioned through a Large Language Model, enabling voice-based Q&A and detailed environmental descriptions.

![Prof. Wen and Xiangru at Nanchang Health Expo](images/news/Nanchang_Health_Expo/ProfWen&Xiangru.jpg)

[cite_start]A core innovation of the concept is its dual-feedback system, which leverages the complementary strengths of haptic and auditory cues.
* [cite_start]**Haptic Feedback**: Navigation cues would be sent as intuitive vibrations via a low-latency haptic wearable device. [cite_start]This method excels at delivering fast, 'what-to-do' commands, such as an urgent alert to dodge an obstacle, ensuring immediate physical safety with minimal cognitive load.
* [cite_start]**Auditory Feedback**: In contrast, auditory feedback provides the crucial 'what-is-there' and 'why' context. [cite_start]An AI-powered voice chat can describe the environment in detail, explaining the nature and location of obstacles and other complex information.

![Xiangru with our porject](images/news/Nanchang_Health_Expo/xiangru.jpg)

[cite_start]This synergy allows users to perform reflexive safety actions through touch while gaining a deeper environmental understanding through sound, creating a much safer and more comprehensive experience than either modality could provide alone.
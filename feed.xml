<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://polyu-taslab.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://polyu-taslab.github.io/" rel="alternate" type="text/html" /><updated>2024-12-25T09:47:26+00:00</updated><id>https://polyu-taslab.github.io/feed.xml</id><title type="html">TAS LAB</title><subtitle>Welcome to the Trustworthy Autonomous Systems Laboratory at the Polytechnic University of Hong Kong. Our research focuses on the development of trustworthy autonomous systems, including autonomous vehicles, drones, and robots.</subtitle><entry><title type="html">Reliable UAV Perception and Perching Solutions in Urban Streets</title><link href="https://polyu-taslab.github.io/2024/12/09/Reliable_UAV_Perception_and_Perching_Solutions_in_Urban_Streets.html" rel="alternate" type="text/html" title="Reliable UAV Perception and Perching Solutions in Urban Streets" /><published>2024-12-09T00:00:00+00:00</published><updated>2024-12-25T09:46:01+00:00</updated><id>https://polyu-taslab.github.io/2024/12/09/Reliable_UAV_Perception_and_Perching_Solutions_in_Urban_Streets</id><content type="html" xml:base="https://polyu-taslab.github.io/2024/12/09/Reliable_UAV_Perception_and_Perching_Solutions_in_Urban_Streets.html"><![CDATA[<p>Develop a comprehensive UAV perception and Perching solution, focusing on the integration of smart streetlight poles with a UAV takeoff, landing, and battery exchange platform</p>

<h2 id="abstract">Abstract</h2>

<p>Develop a comprehensive UAV perception and Perching solution, focusing on the integration of smart streetlight poles with a UAV takeoff, landing, and battery exchange platform</p>
<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/Smart_Street_light_Poles_with_UAV_Airports.png" alt="Team Banner" style="width: 70%; height: auto; object-fit: cover; max-width: 600px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="funding-body">Funding Body</h2>

<p>POLYU AAE(Capstone Project)</p>

<h2 id="researcher">Researcher</h2>

<p><a href="https://polyu-taslab.github.io/members/Wen_Weisong.html">Dr. Weisong Wen</a>, <a href="https://polyu-taslab.github.io/members/ZHAO_Jiaqi.html">Mr. ZHAO Jiaqi</a>, Mr. Li Mingjue, <a href="https://polyu-taslab.github.io/members/hujiahao.html">Mr. HU Jiahao</a>, <a href="https://polyu-taslab.github.io/members/Xiao_Naigui.html">Mr. Xiao Naigui</a>, Mr. FU Chenlei</p>
<h2 id="status">Status</h2>
<p>In progress</p>]]></content><author><name></name></author><category term="Landing," /><category term="UAV" /><category term="Perching," /><category term="UAV" /><category term="Airport" /><summary type="html"><![CDATA[Develop a comprehensive UAV perception and Perching solution, focusing on the integration of smart streetlight poles with a UAV takeoff, landing, and battery exchange platform]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://polyu-taslab.github.io/images/project/Smart_Street_light_Poles_with_UAV_Airports.png" /><media:content medium="image" url="https://polyu-taslab.github.io/images/project/Smart_Street_light_Poles_with_UAV_Airports.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">AI assisted inertial navigation system</title><link href="https://polyu-taslab.github.io/2024/10/14/AI_assisted_inertial_navigation_system.html" rel="alternate" type="text/html" title="AI assisted inertial navigation system" /><published>2024-10-14T00:00:00+00:00</published><updated>2024-12-25T09:46:01+00:00</updated><id>https://polyu-taslab.github.io/2024/10/14/AI_assisted_inertial_navigation_system</id><content type="html" xml:base="https://polyu-taslab.github.io/2024/10/14/AI_assisted_inertial_navigation_system.html"><![CDATA[<p>This project aims to develop a deep learning-based inertial navigation algorithm that utilizes accelerometer, gyroscope, and magnetometer data from smart wearables and smartphones to infer the user’s position and movement trajectory, while providing corresponding confidence levels</p>

<h2 id="introduction">Introduction</h2>
<p>Inertial odometry is a critical technology used in various applications, from robotics and autonomous vehicles to augmented reality (AR) and wearable devices. It involves estimating the position and orientation of an object over time using data from inertial measurement units (IMUs), which typically include accelerometers and gyroscopes. However, traditional inertial odometry systems often face challenges such as sensor noise, bias, and drift, which can lead to cumulative errors and reduced accuracy over time. To address these challenges, AI-aided inertial odometry has emerged as a promising solution, leveraging the power of artificial intelligence to enhance the performance and reliability of inertial navigation systems. By integrating AI techniques such as machine learning and sensor fusion, these systems can intelligently process and interpret IMU data, correcting for errors and improving overall accuracy. AI-aided inertial odometry systems can learn from patterns in sensor data, adapt to different environments, and integrate information from multiple sources, such as cameras and GPS, to provide more robust and precise motion tracking. This advancement not only mitigates the limitations of traditional inertial systems but also opens up new possibilities for applications in complex and dynamic environments where traditional methods may fall short. As AI continues to evolve, its integration with inertial odometry is expected to drive significant innovations across various fields, enhancing the capabilities of autonomous systems and enriching user experiences in wearable devices. <strong>This project aims to develop a deep learning-based inertial navigation algorithm that utilizes accelerometer, gyroscope, and magnetometer data from smart wearables and smartphones to infer the user’s position and movement trajectory, while providing corresponding confidence levels</strong>.</p>

<h2 id="funding-body">Funding Body</h2>
<p>Honor 上海榮耀智慧科技開發有限公司 (Collaborative Research)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/Honorr.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="Inertial" /><category term="Navigation" /><category term="System," /><category term="AI" /><summary type="html"><![CDATA[This project aims to develop a deep learning-based inertial navigation algorithm that utilizes accelerometer, gyroscope, and magnetometer data from smart wearables and smartphones to infer the user’s position and movement trajectory, while providing corresponding confidence levels]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://polyu-taslab.github.io/images/project/Honor.png" /><media:content medium="image" url="https://polyu-taslab.github.io/images/project/Honor.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Development of an Assisted Navigation and Collision Avoidance System using AI and Location-based Service</title><link href="https://polyu-taslab.github.io/2024/04/08/Development_of_an_Assisted_Navigation_and_Collision_Avoidance_System_using_AI_and_Location-based_Service.html" rel="alternate" type="text/html" title="Development of an Assisted Navigation and Collision Avoidance System using AI and Location-based Service" /><published>2024-04-08T00:00:00+00:00</published><updated>2024-12-25T09:46:01+00:00</updated><id>https://polyu-taslab.github.io/2024/04/08/Development_of_an_Assisted_Navigation_and_Collision_Avoidance_System_using_AI_and_Location-based_Service</id><content type="html" xml:base="https://polyu-taslab.github.io/2024/04/08/Development_of_an_Assisted_Navigation_and_Collision_Avoidance_System_using_AI_and_Location-based_Service.html"><![CDATA[<h2 id="abstract">Abstract</h2>

<p>Urban roadways often face challenges such as building-induced obstructions, creating significant blind spots and positioning inaccuracies that hinder safe lane spacing and inter-vehicle distance management. Existing solutions lack cost-effective enhancements for positioning and over-the-horizon collaborative sensing technologies, particularly in dense urban environments.</p>

<p>To address these issues, this project proposes a low-cost, high-precision co-location solution specifically designed for urban canyons. Additionally, a collision avoidance warning application is developed to provide early warnings and emergency interventions for collision risks in blind spots.</p>

<p>The proposed approach integrates advanced methodologies: (1) Lane-level positioning services utilizing Real-Time Kinematic (RTK) and 3D city maps for precise lane-level positioning, (2) Multi-vehicle collaborative sensing leveraging artificial intelligence (AI) and RTK to enable over-the-horizon sensing and early warnings, and (3) Dynamic model optimization using AI to refine positioning accuracy and develop a prototype multi-vehicle collision avoidance system.</p>

<p>The solution achieves lane-level positioning accuracy with an error margin of less than 0.5 meters and provides collision risk warnings at varying inter-vehicle distances. By significantly reducing traffic accidents caused by urban road challenges, this project offers a transformative step toward safer and more reliable urban transportation systems.</p>

<h2 id="funding-body">Funding Body</h2>

<p>Smart Traffic Fund</p>

<h2 id="researcher">Researcher</h2>

<p><a href="https://polyu-taslab.github.io/members/Wen_Weisong.html">Dr. Weisong Wen</a>, Dr. Li-Ta Hsu, Dr. Guohao Zhang, Dr. Jian Liu, <a href="https://polyu-taslab.github.io/members/Huang_Feng.html">Feng Huang</a>, <a href="https://polyu-taslab.github.io/members/liu_xikun.html">Xikun Liu</a>， <a href="https://polyu-taslab.github.io/members/yihan_zhong.html">Yihan Zhong</a>， <a href="https://polyu-taslab.github.io/members/wang_xin.html">Xin Wang</a>, Yuan Li， <a href="https://polyu-taslab.github.io/members/runzhi_hu.html">Runzhi Hu</a></p>

<h2 id="status">Status</h2>

<p>Completed</p>

<h2 id="system-framework">System Framework</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/stf/pipeline.png" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="demonstration">Demonstration</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/stf/demo_gif.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="achievements">Achievements</h2>

<ul>
  <li>Li, Y., Liu, X., Wen, W., Hsu, L. T., Yuan, Y., Bian, G., &amp; Chen, Q. (2024, September). Factor Graph Optimization Based Multi Epoch Ambiguity Resolution for GNSS RTK and its Evaluation in Hong Kong Urban Canyons. In Proceedings of the 37th International Technical Meeting of the Satellite Division of The Institute of Navigation (ION GNSS+ 2024) (pp. 2151-2162).</li>
</ul>]]></content><author><name></name></author><category term="Positioning" /><category term="Services," /><category term="Multi-Vehicle" /><category term="Collaborative" /><category term="Sensing," /><category term="AI" /><category term="aided" /><category term="GNSS," /><category term="GNSS" /><category term="Signal" /><category term="Tracing," /><category term="Sensor" /><category term="Integration" /><summary type="html"><![CDATA[Abstract]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://polyu-taslab.github.io/images/project/stf/demo_gif.gif" /><media:content medium="image" url="https://polyu-taslab.github.io/images/project/stf/demo_gif.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Safe-assured Learning-based Deep SE(3) Motion Joint Planning and Control for Unmanned Aerial Vehicles</title><link href="https://polyu-taslab.github.io/2024/04/08/Safe-assured_Learning-based_Deep_SE(3)_Motion_Joint_Planning_and_Control_for_Unmanned_Aerial_Vehicles.html" rel="alternate" type="text/html" title="Safe-assured Learning-based Deep SE(3) Motion Joint Planning and Control for Unmanned Aerial Vehicles" /><published>2024-04-08T00:00:00+00:00</published><updated>2024-12-25T09:46:01+00:00</updated><id>https://polyu-taslab.github.io/2024/04/08/Safe-assured_Learning-based_Deep_SE(3)_Motion_Joint_Planning_and_Control_for_Unmanned_Aerial_Vehicles</id><content type="html" xml:base="https://polyu-taslab.github.io/2024/04/08/Safe-assured_Learning-based_Deep_SE(3)_Motion_Joint_Planning_and_Control_for_Unmanned_Aerial_Vehicles.html"><![CDATA[<p>PolyU (UGC)</p>

<h2 id="funding-body">Funding Body</h2>

<p>PolyU (UGC)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="Advanced" /><category term="Vehicle" /><category term="Safety" /><category term="Systems," /><category term="Automated" /><category term="Vehicle" /><category term="Operation," /><category term="Motion" /><category term="Planning," /><category term="Navigation," /><category term="Aerial," /><category term="Marine" /><category term="and" /><category term="Surface" /><category term="Intelligent" /><category term="Vehicles" /><summary type="html"><![CDATA[PolyU (UGC)]]></summary></entry><entry><title type="html">Sustainable Window Cleaning for PolyU Jockey Club Innovation Tower with Unmanned Aerial Vehicles (UAV):An Application of Autonomous Systems Enabled Carbon Reduction</title><link href="https://polyu-taslab.github.io/2024/04/01/Sustainable_Window_Cleaning_for_PolyU_Jockey_Club_Innovation_Tower_with_Unmanned_Aerial_Vehicles-(UAV)_An_Application_of_Autonomous_Systems_Enabled_Carbon_Reduction.html" rel="alternate" type="text/html" title="Sustainable Window Cleaning for PolyU Jockey Club Innovation Tower with Unmanned Aerial Vehicles (UAV):An Application of Autonomous Systems Enabled Carbon Reduction" /><published>2024-04-01T00:00:00+00:00</published><updated>2024-12-25T09:46:01+00:00</updated><id>https://polyu-taslab.github.io/2024/04/01/Sustainable_Window_Cleaning_for_PolyU_Jockey_Club_Innovation_Tower_with_Unmanned_Aerial_Vehicles%20(UAV)_An_Application_of_Autonomous_Systems_Enabled_Carbon_Reduction</id><content type="html" xml:base="https://polyu-taslab.github.io/2024/04/01/Sustainable_Window_Cleaning_for_PolyU_Jockey_Club_Innovation_Tower_with_Unmanned_Aerial_Vehicles-(UAV)_An_Application_of_Autonomous_Systems_Enabled_Carbon_Reduction.html"><![CDATA[<p>PolyU (UGC)</p>

<h2 id="funding-body">Funding Body</h2>

<p>PolyU (UGC)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="Unmanned" /><category term="Aerial" /><category term="Vehicle" /><summary type="html"><![CDATA[PolyU (UGC)]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://polyu-taslab.github.io/images/project/uav_clean.png" /><media:content medium="image" url="https://polyu-taslab.github.io/images/project/uav_clean.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Data-driven-assisted GNSS RTK/INS Navigation for Autonomous Systems in Urban Canyons </title><link href="https://polyu-taslab.github.io/2024/01/01/Data-driven-assisted_GNSS_RTK-INS_Navigation_for_Autonomous_Systems_in_Urban_Canyons.html" rel="alternate" type="text/html" title="Data-driven-assisted GNSS RTK/INS Navigation for Autonomous Systems in Urban Canyons " /><published>2024-01-01T00:00:00+00:00</published><updated>2024-12-25T09:46:01+00:00</updated><id>https://polyu-taslab.github.io/2024/01/01/Data-driven-assisted_GNSS_RTK-INS_Navigation_for_Autonomous_Systems_in_Urban_Canyons</id><content type="html" xml:base="https://polyu-taslab.github.io/2024/01/01/Data-driven-assisted_GNSS_RTK-INS_Navigation_for_Autonomous_Systems_in_Urban_Canyons.html"><![CDATA[<h2 id="abstract">Abstract</h2>

<p>The great development of smart urban transportation has led to the booming of the autonomous vehicle industry. Society places great expectations on autonomous driving to enhance safety and efficiency in transportation systems, particularly in challenging environments like urban canyons where precise positioning is crucial. Global Navigation Satellite System (GNSS) positioning modules are the crucial element of autonomous vehicles for absolute positioning. However, the GNSS pseudorange suffers significant biases and degradation due to the multipath and non-line-of-sight (NLOS) errors caused by tall buildings that widely exist in urban canyons. To improve GNSS positioning accuracy in harsh urban canyons, this paper introduces a deep multimodal learning-based approach to correct distorted pseudorange measurements. The vision features of the environment captured by the camera are integrated to form the multimodal network with GNSS measurements. Meanwhile, positional encoding (PE) is proposed to fuse the image features and the satellite position information in higher-dimensional space. The biases are outputted by the network and are used for improved positioning. Compared to existing solutions, extensive test results show that the proposed method achieves improved positioning accuracy ranging from 12\% to 20\%. Additionally, the method proposed in this paper takes only 0.5 seconds to perform epoch-by-epoch pseudorange correction on Jetson Orin Nano, which meets the real-time requirements and shows extremely high practical value. This paper also opensources a dataset under varying time, satellite distributions, and lighting conditions with accurate NLOS labels. We aim to contribute this dataset to the GNSS and deep learning communities, aspiring to establish it as the benchmark for NLOS classification and pseudorange bias correction. The datasets can be accessed at <a href="https://github.com/ebhrz/KLTDataset">github</a>.</p>

<h2 id="funding-body">Funding Body</h2>

<p>PolyU (UGC)</p>

<h2 id="researcher">Researcher</h2>

<p><a href="https://polyu-taslab.github.io/members/Wen_Weisong.html">Dr. Weisong Wen</a>, <a href="https://polyu-taslab.github.io/members/runzhi_hu.html">Runzhi Hu</a>, Dr. Jian Liu, <a href="https://polyu-taslab.github.io/members/yihan_zhong.html">Yihan Zhong</a>, Dr. Ming Xia</p>

<h2 id="status">Status</h2>

<p>Undergoing</p>

<h2 id="system-framework">System Framework</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/Data_driven_structure.png" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="positioning-results">Positioning Results</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/Data_driven_boxplot.png" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="achievements">Achievements</h2>

<ul>
  <li>Dataset Link: <a href="https://github.com/ebhrz/KLTDataset">https://github.com/ebhrz/KLTDataset</a></li>
</ul>]]></content><author><name></name></author><category term="GNSS" /><category term="Positioning," /><category term="NLOS/Multipath" /><category term="Correction," /><category term="Autonomous" /><category term="Driving," /><category term="Positional" /><category term="Encoding," /><category term="Multimodal" /><category term="Network," /><category term="Vision" /><category term="Feature" /><summary type="html"><![CDATA[Abstract]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://polyu-taslab.github.io/images/project/Data_driven_structure.png" /><media:content medium="image" url="https://polyu-taslab.github.io/images/project/Data_driven_structure.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Maximum Consensus Integration of GNSS and LiDAR for Urban Navigation</title><link href="https://polyu-taslab.github.io/2024/01/01/Maximum_Consensus_Integration_of_GNSS_and_LiDAR_for_Urban_Navigation.html" rel="alternate" type="text/html" title="Maximum Consensus Integration of GNSS and LiDAR for Urban Navigation" /><published>2024-01-01T00:00:00+00:00</published><updated>2024-12-25T09:46:01+00:00</updated><id>https://polyu-taslab.github.io/2024/01/01/Maximum_Consensus_Integration_of_GNSS_and_LiDAR_for_Urban_Navigation</id><content type="html" xml:base="https://polyu-taslab.github.io/2024/01/01/Maximum_Consensus_Integration_of_GNSS_and_LiDAR_for_Urban_Navigation.html"><![CDATA[<p>PolyU (UGC)</p>

<h2 id="funding-body">Funding Body</h2>

<p>PolyU (UGC)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="GNSS," /><category term="LIDAR," /><category term="Sensor" /><category term="fusion" /><summary type="html"><![CDATA[PolyU (UGC)]]></summary></entry><entry><title type="html">Multi-robot Collaborative Operations in Lunar Areas for Regolith Processing</title><link href="https://polyu-taslab.github.io/2023/12/03/Multi_robot_Collaborative_Operations_in_Lunar_Areas_for_Regolith_Processing_Project.html" rel="alternate" type="text/html" title="Multi-robot Collaborative Operations in Lunar Areas for Regolith Processing" /><published>2023-12-03T00:00:00+00:00</published><updated>2024-12-25T09:46:01+00:00</updated><id>https://polyu-taslab.github.io/2023/12/03/Multi_robot_Collaborative_Operations_in_Lunar_Areas_for_Regolith_Processing_Project</id><content type="html" xml:base="https://polyu-taslab.github.io/2023/12/03/Multi_robot_Collaborative_Operations_in_Lunar_Areas_for_Regolith_Processing_Project.html"><![CDATA[<h2 id="abstract">Abstract</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/rugged_surface_problem_lunar.jpg" alt="Team Banner" style="width: 50%; height: auto; object-fit: cover; max-width: 350px; margin: 0 auto; border-radius: 15px;" />
</div>

<p>Research conducted on the lunar surface necessitates the utilization of Unmanned Autonomous Systems (UASs) equipped with sophisticated robotic arms designed for the purpose of collecting soil samples. The primary challenges encountered in this endeavor revolve around the need for precise control over the UASs while navigating through rugged and uneven terrain, all while ensuring the utmost safety of the operation. To tackle these challenges head-on, a cutting-edge multi-robot collaboration system has been conceptualized and put forth as a solution. This innovative system leverages the power of Factor Graph Optimization (FGO) to achieve unparalleled accuracy in positioning the UASs during the soil sample collection process.</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/positioning error.jpg" alt="Team Banner" style="width: 50%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<p>Remarkably, the implementation of this collaborative system has yielded remarkable results, boasting a Root Mean Square Error (RMSE) of merely 0.30 meters. This achievement represents a substantial advancement compared to the conventional methods previously employed in similar research endeavors. A pivotal component of this system is the integration of the Leader-Follower Formation Algorithm, which plays a crucial role in facilitating efficient coordination among multiple robots operating simultaneously in the lunar environment.</p>

<p>Looking ahead, the future research trajectory will be centered around further enhancing the collaborative capabilities of the system. This will entail a deep dive into refining the existing Positioning and Leader-Follower algorithms, with a specific focus on optimizing autonomous navigation techniques and streamlining the soil sampling process. Noteworthy features of the system include automated navigation functionalities, advanced soil identification mechanisms, and seamless soil collection procedures facilitated by the utilization of QR codes for precise localization and task allocation purposes.</p>

<h2 id="funding-body">Funding Body</h2>

<p>Research Centre for Deep Space Explorations  (Dec 2023 - Dec 2025)</p>

<h2 id="researcher">Researcher</h2>

<p><a href="https://polyu-taslab.github.io/members/Wen_Weisong.html">Dr. Weisong Wen</a>, Qian Liang</p>

<h2 id="status">Status</h2>

<p>Ongoing</p>

<h2 id="system-framework">System Framework</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/prototype.png" alt="Team Banner" style="width: 60%; height: auto; object-fit: cover; max-width: 320px; margin: 0 auto; border-radius: 15px;" />
</div>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/overall_architecture_lunar.jpg" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="mapping-results">Mapping Results</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/speed_4_combined_arms.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/speedy_clipped_ms_AO.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="achievements">Achievements</h2>]]></content><author><name></name></author><category term="Multi-robot-collaboration," /><category term="MPC," /><category term="mapping," /><category term="Leader-Follower-Formation-Algorithm," /><category term="sensor-fusion," /><category term="LiDAR," /><category term="IMU" /><summary type="html"><![CDATA[Abstract]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://polyu-taslab.github.io/images/project/prototype.png" /><media:content medium="image" url="https://polyu-taslab.github.io/images/project/prototype.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Vehicle-infrastructure Collaboration for Connected Unmanned Ground and Aerial Vehicles in Complex Urban Canyons</title><link href="https://polyu-taslab.github.io/2023/10/15/Vehicle-infrastructure_Collaboration_for_Connected_Unmanned_Ground_and_Aerial_Vehicles_in_Complex_Urban_Canyons.html" rel="alternate" type="text/html" title="Vehicle-infrastructure Collaboration for Connected Unmanned Ground and Aerial Vehicles in Complex Urban Canyons" /><published>2023-10-15T00:00:00+00:00</published><updated>2024-12-25T09:46:01+00:00</updated><id>https://polyu-taslab.github.io/2023/10/15/Vehicle-infrastructure_Collaboration_for_Connected_Unmanned_Ground_and_Aerial_Vehicles_in_Complex_Urban_Canyons</id><content type="html" xml:base="https://polyu-taslab.github.io/2023/10/15/Vehicle-infrastructure_Collaboration_for_Connected_Unmanned_Ground_and_Aerial_Vehicles_in_Complex_Urban_Canyons.html"><![CDATA[<p>PolyU (UGC)</p>

<h2 id="funding-body">Funding Body</h2>

<p>PolyU (UGC)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="Unmanned" /><category term="Ground" /><category term="Vehicle," /><category term="Unmanned" /><category term="Aerial" /><category term="Vehicle," /><category term="Cooperation," /><category term="urban" /><category term="canyons" /><summary type="html"><![CDATA[PolyU (UGC)]]></summary></entry><entry><title type="html">High-precision Vehicle-mounted GNSS/IMU/Camera Fusion Positioning Technology in Complex Urban Environments Based on Factor Graph</title><link href="https://polyu-taslab.github.io/2023/09/30/Research_on_high-precision_vehicle-mounted_GNSS-IMU-Camera_fusion_positioning_technology_in_complex_urban_environments_based_on_factor_graph.html" rel="alternate" type="text/html" title="High-precision Vehicle-mounted GNSS/IMU/Camera Fusion Positioning Technology in Complex Urban Environments Based on Factor Graph" /><published>2023-09-30T00:00:00+00:00</published><updated>2024-12-25T09:46:01+00:00</updated><id>https://polyu-taslab.github.io/2023/09/30/Research_on_high-precision_vehicle-mounted_GNSS-IMU-Camera_fusion_positioning_technology_in_complex_urban_environments_based_on_factor_graph</id><content type="html" xml:base="https://polyu-taslab.github.io/2023/09/30/Research_on_high-precision_vehicle-mounted_GNSS-IMU-Camera_fusion_positioning_technology_in_complex_urban_environments_based_on_factor_graph.html"><![CDATA[<h2 id="abstract">Abstract</h2>

<p>With the widespread use of the global navigation satellite systems (GNSS) in vehicular positioning, real-time kinematic (RTK) positioning has attracted much attention due to its high-precision characteristics. However, in urban canyons, the performance of GNSS-RTK is easily affected by insufficient and low-quality measurements, resulting in unsuccessful ambiguity resolution (AR) and thus large positioning errors. To address this problem, a multi-epoch joint integer AR strategy based on factor graph optimization (FGO) is proposed. This proposed strategy fully utilizes the temporal correlation between all ambiguities in consecutive epochs when conducting AR, adopting soft single-differenced (SD) ambiguity equality constraint instead of ambiguity parameter merging. A complex vehicle-borne low-cost GNSS dataset collected in the urban canyon of Hong Kong was used. The results show that the proposed method can increase the positioning accuracy compared with traditional AR strategy.</p>

<h2 id="funding-body">Funding Body</h2>

<p>Tencent Dadi Tongtu (Beijing) Technology Co., Ltd 騰訊大地通途北京科技有限公司</p>

<h2 id="researcher">Researcher</h2>
<p>Yuan Li, <a href="https://polyu-taslab.github.io/members/liu_xikun.html">Xikun Liu</a>, <a href="https://polyu-taslab.github.io/members/Wen_Weisong.html">Weisong Wen</a>, Li-ta Hsu, Yilong Yuan, Guangyu Bian, Qiaoyun Chen</p>

<h2 id="status">Status</h2>

<p>Published</p>

<h2 id="system-framework">System Framework</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/tencent/framework.jpg" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="positioning-results">Positioning Results</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/tencent/result.png" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/tencent/trajectory.png" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="achievements">Achievements</h2>

<ul>
  <li>Li, Yuan, et al. “Factor Graph Optimization Based Multi Epoch Ambiguity Resolution for GNSS RTK and its Evaluation in Hong Kong Urban Canyons.” Proceedings of the 37th International Technical Meeting of the Satellite Division of The Institute of Navigation (ION GNSS+ 2024). 2024.</li>
</ul>]]></content><author><name></name></author><category term="Global" /><category term="navigation" /><category term="satellite" /><category term="system," /><category term="Real-time" /><category term="kinematic" /><category term="positioning," /><category term="Factor" /><category term="graph" /><category term="optimization," /><category term="Multi‑epoch" /><category term="ambiguity" /><category term="resolution," /><category term="Urban" /><category term="canyons" /><summary type="html"><![CDATA[Abstract]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://polyu-taslab.github.io/images/project/Vision_aided_GNSS_RTK/framework.png" /><media:content medium="image" url="https://polyu-taslab.github.io/images/project/Vision_aided_GNSS_RTK/framework.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>
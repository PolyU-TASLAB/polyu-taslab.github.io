<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://polyu-taslab.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://polyu-taslab.github.io/" rel="alternate" type="text/html" /><updated>2024-12-23T08:17:28+00:00</updated><id>https://polyu-taslab.github.io/feed.xml</id><title type="html">TAS LAB</title><subtitle>Welcome to the Trustworthy Autonomous Systems Laboratory at the Polytechnic University of Hong Kong. Our research focuses on the development of trustworthy autonomous systems, including autonomous vehicles, drones, and robots.</subtitle><entry><title type="html">Reliable UAV Perception and Perching Solutions in Urban Streets</title><link href="https://polyu-taslab.github.io/2024/12/09/Reliable_UAV_Perception_and_Perching_Solutions_in_Urban_Streets.html" rel="alternate" type="text/html" title="Reliable UAV Perception and Perching Solutions in Urban Streets" /><published>2024-12-09T00:00:00+00:00</published><updated>2024-12-23T08:15:22+00:00</updated><id>https://polyu-taslab.github.io/2024/12/09/Reliable_UAV_Perception_and_Perching_Solutions_in_Urban_Streets</id><content type="html" xml:base="https://polyu-taslab.github.io/2024/12/09/Reliable_UAV_Perception_and_Perching_Solutions_in_Urban_Streets.html"><![CDATA[<p>Develop a comprehensive UAV perception and Perching solution, focusing on the integration of smart streetlight poles with a UAV takeoff, landing, and battery exchange platform</p>

<h2 id="abstract">Abstract</h2>

<p>Develop a comprehensive UAV perception and Perching solution, focusing on the integration of smart streetlight poles with a UAV takeoff, landing, and battery exchange platform</p>
<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/Smart_Street_light_Poles_with_UAV_Airports.png" alt="Team Banner" style="width: 70%; height: auto; object-fit: cover; max-width: 600px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="funding-body">Funding Body</h2>

<p>POLYU AAE(Capstone Project)</p>

<h2 id="researcher">Researcher</h2>

<p><a href="https://polyu-taslab.github.io/members/Wen_Weisong.html">Dr. Weisong Wen</a>, <a href="https://polyu-taslab.github.io/members/ZHAO_Jiaqi.html">Mr. ZHAO Jiaqi</a>, Mr. Li Mingjue, <a href="https://polyu-taslab.github.io/members/hujiahao.html">Mr. HU Jiahao</a>, <a href="https://polyu-taslab.github.io/members/Xiao_Naigui.html">Mr. Xiao Naigui</a>, Mr. FU Chenlei</p>
<h2 id="status">Status</h2>
<p>In progress</p>]]></content><author><name></name></author><category term="Landing," /><category term="UAV" /><category term="Perching," /><category term="UAV" /><category term="Airport" /><summary type="html"><![CDATA[Develop a comprehensive UAV perception and Perching solution, focusing on the integration of smart streetlight poles with a UAV takeoff, landing, and battery exchange platform]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://polyu-taslab.github.io/images/project/Smart_Street_light_Poles_with_UAV_Airports.png" /><media:content medium="image" url="https://polyu-taslab.github.io/images/project/Smart_Street_light_Poles_with_UAV_Airports.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">AI assisted inertial navigation system</title><link href="https://polyu-taslab.github.io/2024/10/14/AI_assisted_inertial_navigation_system.html" rel="alternate" type="text/html" title="AI assisted inertial navigation system" /><published>2024-10-14T00:00:00+00:00</published><updated>2024-12-23T08:15:22+00:00</updated><id>https://polyu-taslab.github.io/2024/10/14/AI_assisted_inertial_navigation_system</id><content type="html" xml:base="https://polyu-taslab.github.io/2024/10/14/AI_assisted_inertial_navigation_system.html"><![CDATA[<p>This project aims to develop a deep learning-based inertial navigation algorithm that utilizes accelerometer, gyroscope, and magnetometer data from smart wearables and smartphones to infer the user’s position and movement trajectory, while providing corresponding confidence levels</p>

<h2 id="introduction">Introduction</h2>
<p>Inertial odometry is a critical technology used in various applications, from robotics and autonomous vehicles to augmented reality (AR) and wearable devices. It involves estimating the position and orientation of an object over time using data from inertial measurement units (IMUs), which typically include accelerometers and gyroscopes. However, traditional inertial odometry systems often face challenges such as sensor noise, bias, and drift, which can lead to cumulative errors and reduced accuracy over time. To address these challenges, AI-aided inertial odometry has emerged as a promising solution, leveraging the power of artificial intelligence to enhance the performance and reliability of inertial navigation systems. By integrating AI techniques such as machine learning and sensor fusion, these systems can intelligently process and interpret IMU data, correcting for errors and improving overall accuracy. AI-aided inertial odometry systems can learn from patterns in sensor data, adapt to different environments, and integrate information from multiple sources, such as cameras and GPS, to provide more robust and precise motion tracking. This advancement not only mitigates the limitations of traditional inertial systems but also opens up new possibilities for applications in complex and dynamic environments where traditional methods may fall short. As AI continues to evolve, its integration with inertial odometry is expected to drive significant innovations across various fields, enhancing the capabilities of autonomous systems and enriching user experiences in wearable devices. <strong>This project aims to develop a deep learning-based inertial navigation algorithm that utilizes accelerometer, gyroscope, and magnetometer data from smart wearables and smartphones to infer the user’s position and movement trajectory, while providing corresponding confidence levels</strong>.</p>

<h2 id="funding-body">Funding Body</h2>
<p>Honor 上海榮耀智慧科技開發有限公司 (Collaborative Research)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/Honorr.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="Inertial" /><category term="Navigation" /><category term="System," /><category term="AI" /><summary type="html"><![CDATA[This project aims to develop a deep learning-based inertial navigation algorithm that utilizes accelerometer, gyroscope, and magnetometer data from smart wearables and smartphones to infer the user’s position and movement trajectory, while providing corresponding confidence levels]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://polyu-taslab.github.io/images/project/Honor.png" /><media:content medium="image" url="https://polyu-taslab.github.io/images/project/Honor.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Development of an Assisted Navigation and Collision Avoidance System using AI and Location-based Service</title><link href="https://polyu-taslab.github.io/2024/04/08/Development_of_an_Assisted_Navigation_and_Collision_Avoidance_System_using_AI_and_Location-based_Service.html" rel="alternate" type="text/html" title="Development of an Assisted Navigation and Collision Avoidance System using AI and Location-based Service" /><published>2024-04-08T00:00:00+00:00</published><updated>2024-12-23T08:15:22+00:00</updated><id>https://polyu-taslab.github.io/2024/04/08/Development_of_an_Assisted_Navigation_and_Collision_Avoidance_System_using_AI_and_Location-based_Service</id><content type="html" xml:base="https://polyu-taslab.github.io/2024/04/08/Development_of_an_Assisted_Navigation_and_Collision_Avoidance_System_using_AI_and_Location-based_Service.html"><![CDATA[<p>Transport Department</p>

<h2 id="funding-body">Funding Body</h2>

<p>Transport Department</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="AI," /><category term="3DMA-RTK," /><category term="Vehicle" /><category term="Risk" /><category term="Prediction," /><category term="Collision" /><category term="Avoidance" /><summary type="html"><![CDATA[Transport Department]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://polyu-taslab.github.io/images/project/stf.png" /><media:content medium="image" url="https://polyu-taslab.github.io/images/project/stf.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Safe-assured Learning-based Deep SE(3) Motion Joint Planning and Control for Unmanned Aerial Vehicles</title><link href="https://polyu-taslab.github.io/2024/04/08/Safe-assured_Learning-based_Deep_SE(3)_Motion_Joint_Planning_and_Control_for_Unmanned_Aerial_Vehicles.html" rel="alternate" type="text/html" title="Safe-assured Learning-based Deep SE(3) Motion Joint Planning and Control for Unmanned Aerial Vehicles" /><published>2024-04-08T00:00:00+00:00</published><updated>2024-12-23T08:15:22+00:00</updated><id>https://polyu-taslab.github.io/2024/04/08/Safe-assured_Learning-based_Deep_SE(3)_Motion_Joint_Planning_and_Control_for_Unmanned_Aerial_Vehicles</id><content type="html" xml:base="https://polyu-taslab.github.io/2024/04/08/Safe-assured_Learning-based_Deep_SE(3)_Motion_Joint_Planning_and_Control_for_Unmanned_Aerial_Vehicles.html"><![CDATA[<p>PolyU (UGC)</p>

<h2 id="funding-body">Funding Body</h2>

<p>PolyU (UGC)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="Advanced" /><category term="Vehicle" /><category term="Safety" /><category term="Systems," /><category term="Automated" /><category term="Vehicle" /><category term="Operation," /><category term="Motion" /><category term="Planning," /><category term="Navigation," /><category term="Aerial," /><category term="Marine" /><category term="and" /><category term="Surface" /><category term="Intelligent" /><category term="Vehicles" /><summary type="html"><![CDATA[PolyU (UGC)]]></summary></entry><entry><title type="html">Sustainable Window Cleaning for PolyU Jockey Club Innovation Tower with Unmanned Aerial Vehicles (UAV):An Application of Autonomous Systems Enabled Carbon Reduction</title><link href="https://polyu-taslab.github.io/2024/04/01/Sustainable_Window_Cleaning_for_PolyU_Jockey_Club_Innovation_Tower_with_Unmanned_Aerial_Vehicles-(UAV)_An_Application_of_Autonomous_Systems_Enabled_Carbon_Reduction.html" rel="alternate" type="text/html" title="Sustainable Window Cleaning for PolyU Jockey Club Innovation Tower with Unmanned Aerial Vehicles (UAV):An Application of Autonomous Systems Enabled Carbon Reduction" /><published>2024-04-01T00:00:00+00:00</published><updated>2024-12-23T08:15:22+00:00</updated><id>https://polyu-taslab.github.io/2024/04/01/Sustainable_Window_Cleaning_for_PolyU_Jockey_Club_Innovation_Tower_with_Unmanned_Aerial_Vehicles%20(UAV)_An_Application_of_Autonomous_Systems_Enabled_Carbon_Reduction</id><content type="html" xml:base="https://polyu-taslab.github.io/2024/04/01/Sustainable_Window_Cleaning_for_PolyU_Jockey_Club_Innovation_Tower_with_Unmanned_Aerial_Vehicles-(UAV)_An_Application_of_Autonomous_Systems_Enabled_Carbon_Reduction.html"><![CDATA[<p>PolyU (UGC)</p>

<h2 id="funding-body">Funding Body</h2>

<p>PolyU (UGC)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="Unmanned" /><category term="Aerial" /><category term="Vehicle" /><summary type="html"><![CDATA[PolyU (UGC)]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://polyu-taslab.github.io/images/project/uav_clean.png" /><media:content medium="image" url="https://polyu-taslab.github.io/images/project/uav_clean.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Data-driven-assisted GNSS RTK/INS Navigation for Autonomous Systems in Urban Canyons</title><link href="https://polyu-taslab.github.io/2024/01/01/Data-driven-assisted_GNSS_RTK-INS_Navigation_for_Autonomous_Systems_in_Urban_Canyons.html" rel="alternate" type="text/html" title="Data-driven-assisted GNSS RTK/INS Navigation for Autonomous Systems in Urban Canyons" /><published>2024-01-01T00:00:00+00:00</published><updated>2024-12-23T08:15:22+00:00</updated><id>https://polyu-taslab.github.io/2024/01/01/Data-driven-assisted_GNSS_RTK-INS_Navigation_for_Autonomous_Systems_in_Urban_Canyons</id><content type="html" xml:base="https://polyu-taslab.github.io/2024/01/01/Data-driven-assisted_GNSS_RTK-INS_Navigation_for_Autonomous_Systems_in_Urban_Canyons.html"><![CDATA[<p>PolyU (UGC)</p>

<h2 id="funding-body">Funding Body</h2>

<p>PolyU (UGC)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="INS," /><category term="urban" /><category term="canyons," /><category term="GNSS," /><category term="RTK," /><category term="Autonomous" /><category term="System" /><summary type="html"><![CDATA[PolyU (UGC)]]></summary></entry><entry><title type="html">Maximum Consensus Integration of GNSS and LiDAR for Urban Navigation</title><link href="https://polyu-taslab.github.io/2024/01/01/Maximum_Consensus_Integration_of_GNSS_and_LiDAR_for_Urban_Navigation.html" rel="alternate" type="text/html" title="Maximum Consensus Integration of GNSS and LiDAR for Urban Navigation" /><published>2024-01-01T00:00:00+00:00</published><updated>2024-12-23T08:15:22+00:00</updated><id>https://polyu-taslab.github.io/2024/01/01/Maximum_Consensus_Integration_of_GNSS_and_LiDAR_for_Urban_Navigation</id><content type="html" xml:base="https://polyu-taslab.github.io/2024/01/01/Maximum_Consensus_Integration_of_GNSS_and_LiDAR_for_Urban_Navigation.html"><![CDATA[<p>PolyU (UGC)</p>

<h2 id="funding-body">Funding Body</h2>

<p>PolyU (UGC)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="GNSS," /><category term="LIDAR," /><category term="Sensor" /><category term="fusion" /><summary type="html"><![CDATA[PolyU (UGC)]]></summary></entry><entry><title type="html">Multi-robot Collaborative Operations in Lunar Areas for Regolith Processing</title><link href="https://polyu-taslab.github.io/2023/12/03/Multi-robot_Collaborative_Operations_in_Lunar_Areas_for_Regolith_Processing_Project.html" rel="alternate" type="text/html" title="Multi-robot Collaborative Operations in Lunar Areas for Regolith Processing" /><published>2023-12-03T00:00:00+00:00</published><updated>2024-12-23T08:15:22+00:00</updated><id>https://polyu-taslab.github.io/2023/12/03/Multi-robot_Collaborative_Operations_in_Lunar_Areas_for_Regolith_Processing_Project</id><content type="html" xml:base="https://polyu-taslab.github.io/2023/12/03/Multi-robot_Collaborative_Operations_in_Lunar_Areas_for_Regolith_Processing_Project.html"><![CDATA[<h2 id="abstract">Abstract</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/rugged_surface_problem_lunar.jpg" alt="Team Banner" style="width: 50%; height: auto; object-fit: cover; max-width: 350px; margin: 0 auto; border-radius: 15px;" />
</div>

<p>Research conducted on the lunar surface necessitates the utilization of Unmanned Autonomous Systems (UASs) equipped with sophisticated robotic arms designed for the purpose of collecting soil samples. The primary challenges encountered in this endeavor revolve around the need for precise control over the UASs while navigating through rugged and uneven terrain, all while ensuring the utmost safety of the operation. To tackle these challenges head-on, a cutting-edge multi-robot collaboration system has been conceptualized and put forth as a solution. This innovative system leverages the power of Factor Graph Optimization (FGO) to achieve unparalleled accuracy in positioning the UASs during the soil sample collection process.</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/positioning error.jpg" alt="Team Banner" style="width: 50%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<p>Remarkably, the implementation of this collaborative system has yielded remarkable results, boasting a Root Mean Square Error (RMSE) of merely 0.30 meters. This achievement represents a substantial advancement compared to the conventional methods previously employed in similar research endeavors. A pivotal component of this system is the integration of the Leader-Follower Formation Algorithm, which plays a crucial role in facilitating efficient coordination among multiple robots operating simultaneously in the lunar environment.</p>

<p>Looking ahead, the future research trajectory will be centered around further enhancing the collaborative capabilities of the system. This will entail a deep dive into refining the existing Positioning and Leader-Follower algorithms, with a specific focus on optimizing autonomous navigation techniques and streamlining the soil sampling process. Noteworthy features of the system include automated navigation functionalities, advanced soil identification mechanisms, and seamless soil collection procedures facilitated by the utilization of QR codes for precise localization and task allocation purposes.</p>

<h2 id="funding-body">Funding Body</h2>

<p>Research Centre for Deep Space Explorations  (Dec 2023 - Dec 2025)</p>

<h2 id="researcher">Researcher</h2>

<p><a href="https://polyu-taslab.github.io/members/Wen_Weisong.html">Dr. Weisong Wen</a>, Qian Liang</p>

<h2 id="status">Status</h2>

<p>Ongoing</p>

<h2 id="system-framework">System Framework</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/prototype.png" alt="Team Banner" style="width: 60%; height: auto; object-fit: cover; max-width: 320px; margin: 0 auto; border-radius: 15px;" />
</div>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/overall_architecture_lunar.jpg" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="mapping-results">Mapping Results</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/speed_4_combined_arms.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/speedy_clipped_ms_AO.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="achievements">Achievements</h2>]]></content><author><name></name></author><category term="Multi-robot-collaboration," /><category term="MPC," /><category term="mapping," /><category term="Leader-Follower-Formation-Algorithm," /><category term="sensor-fusion," /><category term="LiDAR," /><category term="IMU" /><summary type="html"><![CDATA[Abstract Research conducted on the lunar surface necessitates the utilization of Unmanned Autonomous Systems (UASs) equipped with sophisticated robotic arms designed for the purpose of collecting soil samples. The primary challenges encountered in this endeavor revolve around the need for precise control over the UASs while navigating through rugged and uneven terrain, all while ensuring the utmost safety of the operation. To tackle these challenges head-on, a cutting-edge multi-robot collaboration system has been conceptualized and put forth as a solution. This innovative system leverages the power of Factor Graph Optimization (FGO) to achieve unparalleled accuracy in positioning the UASs during the soil sample collection process. Remarkably, the implementation of this collaborative system has yielded remarkable results, boasting a Root Mean Square Error (RMSE) of merely 0.30 meters. This achievement represents a substantial advancement compared to the conventional methods previously employed in similar research endeavors. A pivotal component of this system is the integration of the Leader-Follower Formation Algorithm, which plays a crucial role in facilitating efficient coordination among multiple robots operating simultaneously in the lunar environment. Looking ahead, the future research trajectory will be centered around further enhancing the collaborative capabilities of the system. This will entail a deep dive into refining the existing Positioning and Leader-Follower algorithms, with a specific focus on optimizing autonomous navigation techniques and streamlining the soil sampling process. Noteworthy features of the system include automated navigation functionalities, advanced soil identification mechanisms, and seamless soil collection procedures facilitated by the utilization of QR codes for precise localization and task allocation purposes. Funding Body Research Centre for Deep Space Explorations (Dec 2023 - Dec 2025) Researcher Dr. Weisong Wen, Qian Liang Status Ongoing System Framework Mapping Results Achievements]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://polyu-taslab.github.io/images/project/prototype.png" /><media:content medium="image" url="https://polyu-taslab.github.io/images/project/prototype.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Vehicle-infrastructure Collaboration for Connected Unmanned Ground and Aerial Vehicles in Complex Urban Canyons</title><link href="https://polyu-taslab.github.io/2023/10/15/Vehicle-infrastructure_Collaboration_for_Connected_Unmanned_Ground_and_Aerial_Vehicles_in_Complex_Urban_Canyons.html" rel="alternate" type="text/html" title="Vehicle-infrastructure Collaboration for Connected Unmanned Ground and Aerial Vehicles in Complex Urban Canyons" /><published>2023-10-15T00:00:00+00:00</published><updated>2024-12-23T08:15:22+00:00</updated><id>https://polyu-taslab.github.io/2023/10/15/Vehicle-infrastructure_Collaboration_for_Connected_Unmanned_Ground_and_Aerial_Vehicles_in_Complex_Urban_Canyons</id><content type="html" xml:base="https://polyu-taslab.github.io/2023/10/15/Vehicle-infrastructure_Collaboration_for_Connected_Unmanned_Ground_and_Aerial_Vehicles_in_Complex_Urban_Canyons.html"><![CDATA[<p>PolyU (UGC)</p>

<h2 id="funding-body">Funding Body</h2>

<p>PolyU (UGC)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="Unmanned" /><category term="Ground" /><category term="Vehicle," /><category term="Unmanned" /><category term="Aerial" /><category term="Vehicle," /><category term="Cooperation," /><category term="urban" /><category term="canyons" /><summary type="html"><![CDATA[PolyU (UGC)]]></summary></entry><entry><title type="html">Research on high-precision vehicle-mounted GNSS/IMU/Camera fusion positioning technology in complex urban environments based on factor graph</title><link href="https://polyu-taslab.github.io/2023/09/30/Research_on_high-precision_vehicle-mounted_GNSS-IMU-Camera_fusion_positioning_technology_in_complex_urban_environments_based_on_factor_graph.html" rel="alternate" type="text/html" title="Research on high-precision vehicle-mounted GNSS/IMU/Camera fusion positioning technology in complex urban environments based on factor graph" /><published>2023-09-30T00:00:00+00:00</published><updated>2024-12-23T08:15:22+00:00</updated><id>https://polyu-taslab.github.io/2023/09/30/Research_on_high-precision_vehicle-mounted_GNSS-IMU-Camera_fusion_positioning_technology_in_complex_urban_environments_based_on_factor_graph</id><content type="html" xml:base="https://polyu-taslab.github.io/2023/09/30/Research_on_high-precision_vehicle-mounted_GNSS-IMU-Camera_fusion_positioning_technology_in_complex_urban_environments_based_on_factor_graph.html"><![CDATA[<p>Tencent Dadi Tongtu (Beijing) Technology Co., Ltd 騰訊大地通途北京科技有限公司</p>

<h2 id="funding-body">Funding Body</h2>

<p>Tencent Dadi Tongtu (Beijing) Technology Co., Ltd 騰訊大地通途北京科技有限公司</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="Sensor" /><category term="fusion," /><category term="FGO," /><category term="Localization" /><summary type="html"><![CDATA[Tencent Dadi Tongtu (Beijing) Technology Co., Ltd 騰訊大地通途北京科技有限公司]]></summary></entry></feed>
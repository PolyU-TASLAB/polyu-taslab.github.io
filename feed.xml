<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://polyu-taslab.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://polyu-taslab.github.io/" rel="alternate" type="text/html" /><updated>2024-12-16T11:33:53+00:00</updated><id>https://polyu-taslab.github.io/feed.xml</id><title type="html">TAS LAB</title><subtitle>Welcome to the Trustworthy Autonomous Systems Laboratory at the Polytechnic University of Hong Kong. Our research focuses on the development of trustworthy autonomous systems, including autonomous vehicles, drones, and robots.</subtitle><entry><title type="html">Reliable UAV Perception and Perching Solutions in Urban Streets</title><link href="https://polyu-taslab.github.io/2024/12/09/Reliable_UAV_Perception_and_Perching_Solutions_in_Urban_Streets.html" rel="alternate" type="text/html" title="Reliable UAV Perception and Perching Solutions in Urban Streets" /><published>2024-12-09T00:00:00+00:00</published><updated>2024-12-16T11:32:04+00:00</updated><id>https://polyu-taslab.github.io/2024/12/09/Reliable_UAV_Perception_and_Perching_Solutions_in_Urban_Streets</id><content type="html" xml:base="https://polyu-taslab.github.io/2024/12/09/Reliable_UAV_Perception_and_Perching_Solutions_in_Urban_Streets.html"><![CDATA[<h2 id="abstract">Abstract</h2>

<p>Develop a comprehensive UAV perception and Perching solution, focusing on the integration of smart streetlight poles with a UAV takeoff, landing, and battery exchange platform</p>
<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/Smart_Street_light_Poles_with_UAV_Airports.png" alt="Team Banner" style="width: 70%; height: auto; object-fit: cover; max-width: 600px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="funding-body">Funding Body</h2>

<p>POLYU AAE(Capstone Project)</p>

<h2 id="researcher">Researcher</h2>

<p><a href="https://polyu-taslab.github.io/members/Wen_Weisong.html">Dr. Weisong Wen</a>, <a href="https://polyu-taslab.github.io/members/ZHAO_Jiaqi.html">Mr. ZHAO Jiaqi</a>, Mr. Li Mingjue, <a href="https://polyu-taslab.github.io/members/hujiahao.html">Mr. HU Jiahao</a>, <a href="https://polyu-taslab.github.io/members/Xiao_Naigui.html">Mr. Xiao Naigui</a>, Mr. FU Chenlei</p>
<h2 id="status">Status</h2>
<p>In progress</p>]]></content><author><name></name></author><category term="Landing," /><category term="UAV" /><category term="Perching," /><category term="UAV" /><category term="Airport" /><summary type="html"><![CDATA[Abstract]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://polyu-taslab.github.io/images/project/Smart_Street_light_Poles_with_UAV_Airports.png" /><media:content medium="image" url="https://polyu-taslab.github.io/images/project/Smart_Street_light_Poles_with_UAV_Airports.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">AI assisted inertial navigation system</title><link href="https://polyu-taslab.github.io/2024/10/14/AI_assisted_inertial_navigation_system.html" rel="alternate" type="text/html" title="AI assisted inertial navigation system" /><published>2024-10-14T00:00:00+00:00</published><updated>2024-12-16T11:32:04+00:00</updated><id>https://polyu-taslab.github.io/2024/10/14/AI_assisted_inertial_navigation_system</id><content type="html" xml:base="https://polyu-taslab.github.io/2024/10/14/AI_assisted_inertial_navigation_system.html"><![CDATA[<p>This project aims to develop a deep learning-based inertial navigation algorithm that utilizes accelerometer, gyroscope, and magnetometer data from smart wearables and smartphones to infer the user’s position and movement trajectory, while providing corresponding confidence levels</p>

<h2 id="introduction">Introduction</h2>
<p>Inertial odometry is a critical technology used in various applications, from robotics and autonomous vehicles to augmented reality (AR) and wearable devices. It involves estimating the position and orientation of an object over time using data from inertial measurement units (IMUs), which typically include accelerometers and gyroscopes. However, traditional inertial odometry systems often face challenges such as sensor noise, bias, and drift, which can lead to cumulative errors and reduced accuracy over time. To address these challenges, AI-aided inertial odometry has emerged as a promising solution, leveraging the power of artificial intelligence to enhance the performance and reliability of inertial navigation systems. By integrating AI techniques such as machine learning and sensor fusion, these systems can intelligently process and interpret IMU data, correcting for errors and improving overall accuracy. AI-aided inertial odometry systems can learn from patterns in sensor data, adapt to different environments, and integrate information from multiple sources, such as cameras and GPS, to provide more robust and precise motion tracking. This advancement not only mitigates the limitations of traditional inertial systems but also opens up new possibilities for applications in complex and dynamic environments where traditional methods may fall short. As AI continues to evolve, its integration with inertial odometry is expected to drive significant innovations across various fields, enhancing the capabilities of autonomous systems and enriching user experiences in wearable devices. <strong>This project aims to develop a deep learning-based inertial navigation algorithm that utilizes accelerometer, gyroscope, and magnetometer data from smart wearables and smartphones to infer the user’s position and movement trajectory, while providing corresponding confidence levels</strong>.</p>

<h2 id="funding-body">Funding Body</h2>
<p>Honor 上海榮耀智慧科技開發有限公司 (Collaborative Research)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/Honorr.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="Inertial" /><category term="Navigation" /><category term="System," /><category term="AI" /><summary type="html"><![CDATA[This project aims to develop a deep learning-based inertial navigation algorithm that utilizes accelerometer, gyroscope, and magnetometer data from smart wearables and smartphones to infer the user’s position and movement trajectory, while providing corresponding confidence levels]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://polyu-taslab.github.io/images/project/Honor.png" /><media:content medium="image" url="https://polyu-taslab.github.io/images/project/Honor.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Development of an Assisted Navigation and Collision Avoidance System using AI and Location-based Service</title><link href="https://polyu-taslab.github.io/2024/04/08/Development_of_an_Assisted_Navigation_and_Collision_Avoidance_System_using_AI_and_Location-based_Service.html" rel="alternate" type="text/html" title="Development of an Assisted Navigation and Collision Avoidance System using AI and Location-based Service" /><published>2024-04-08T00:00:00+00:00</published><updated>2024-12-16T11:32:04+00:00</updated><id>https://polyu-taslab.github.io/2024/04/08/Development_of_an_Assisted_Navigation_and_Collision_Avoidance_System_using_AI_and_Location-based_Service</id><content type="html" xml:base="https://polyu-taslab.github.io/2024/04/08/Development_of_an_Assisted_Navigation_and_Collision_Avoidance_System_using_AI_and_Location-based_Service.html"><![CDATA[<p>Transport Department</p>

<h2 id="funding-body">Funding Body</h2>

<p>Transport Department</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="AI," /><category term="3DMA-RTK," /><category term="Vehicle" /><category term="Risk" /><category term="Prediction," /><category term="Collision" /><category term="Avoidance" /><summary type="html"><![CDATA[Transport Department]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://polyu-taslab.github.io/images/project/stf.png" /><media:content medium="image" url="https://polyu-taslab.github.io/images/project/stf.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Safe-assured Learning-based Deep SE(3) Motion Joint Planning and Control for Unmanned Aerial Vehicles</title><link href="https://polyu-taslab.github.io/2024/04/08/Safe-assured_Learning-based_Deep_SE(3)_Motion_Joint_Planning_and_Control_for_Unmanned_Aerial_Vehicles.html" rel="alternate" type="text/html" title="Safe-assured Learning-based Deep SE(3) Motion Joint Planning and Control for Unmanned Aerial Vehicles" /><published>2024-04-08T00:00:00+00:00</published><updated>2024-12-16T11:32:04+00:00</updated><id>https://polyu-taslab.github.io/2024/04/08/Safe-assured_Learning-based_Deep_SE(3)_Motion_Joint_Planning_and_Control_for_Unmanned_Aerial_Vehicles</id><content type="html" xml:base="https://polyu-taslab.github.io/2024/04/08/Safe-assured_Learning-based_Deep_SE(3)_Motion_Joint_Planning_and_Control_for_Unmanned_Aerial_Vehicles.html"><![CDATA[<p>PolyU (UGC)</p>

<h2 id="funding-body">Funding Body</h2>

<p>PolyU (UGC)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="Advanced" /><category term="Vehicle" /><category term="Safety" /><category term="Systems," /><category term="Automated" /><category term="Vehicle" /><category term="Operation," /><category term="Motion" /><category term="Planning," /><category term="Navigation," /><category term="Aerial," /><category term="Marine" /><category term="and" /><category term="Surface" /><category term="Intelligent" /><category term="Vehicles" /><summary type="html"><![CDATA[PolyU (UGC)]]></summary></entry><entry><title type="html">Sustainable Window Cleaning for PolyU Jockey Club Innovation Tower with Unmanned Aerial Vehicles (UAV):An Application of Autonomous Systems Enabled Carbon Reduction</title><link href="https://polyu-taslab.github.io/2024/04/01/Sustainable_Window_Cleaning_for_PolyU_Jockey_Club_Innovation_Tower_with_Unmanned_Aerial_Vehicles-(UAV)_An_Application_of_Autonomous_Systems_Enabled_Carbon_Reduction.html" rel="alternate" type="text/html" title="Sustainable Window Cleaning for PolyU Jockey Club Innovation Tower with Unmanned Aerial Vehicles (UAV):An Application of Autonomous Systems Enabled Carbon Reduction" /><published>2024-04-01T00:00:00+00:00</published><updated>2024-12-16T11:32:04+00:00</updated><id>https://polyu-taslab.github.io/2024/04/01/Sustainable_Window_Cleaning_for_PolyU_Jockey_Club_Innovation_Tower_with_Unmanned_Aerial_Vehicles%20(UAV)_An_Application_of_Autonomous_Systems_Enabled_Carbon_Reduction</id><content type="html" xml:base="https://polyu-taslab.github.io/2024/04/01/Sustainable_Window_Cleaning_for_PolyU_Jockey_Club_Innovation_Tower_with_Unmanned_Aerial_Vehicles-(UAV)_An_Application_of_Autonomous_Systems_Enabled_Carbon_Reduction.html"><![CDATA[<p>PolyU (UGC)</p>

<h2 id="funding-body">Funding Body</h2>

<p>PolyU (UGC)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="Unmanned" /><category term="Aerial" /><category term="Vehicle" /><summary type="html"><![CDATA[PolyU (UGC)]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://polyu-taslab.github.io/images/project/uav_clean.png" /><media:content medium="image" url="https://polyu-taslab.github.io/images/project/uav_clean.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Data-driven-assisted GNSS RTK/INS Navigation for Autonomous Systems in Urban Canyons</title><link href="https://polyu-taslab.github.io/2024/01/01/Data-driven-assisted_GNSS_RTK-INS_Navigation_for_Autonomous_Systems_in_Urban_Canyons.html" rel="alternate" type="text/html" title="Data-driven-assisted GNSS RTK/INS Navigation for Autonomous Systems in Urban Canyons" /><published>2024-01-01T00:00:00+00:00</published><updated>2024-12-16T11:32:04+00:00</updated><id>https://polyu-taslab.github.io/2024/01/01/Data-driven-assisted_GNSS_RTK-INS_Navigation_for_Autonomous_Systems_in_Urban_Canyons</id><content type="html" xml:base="https://polyu-taslab.github.io/2024/01/01/Data-driven-assisted_GNSS_RTK-INS_Navigation_for_Autonomous_Systems_in_Urban_Canyons.html"><![CDATA[<p>PolyU (UGC)</p>

<h2 id="funding-body">Funding Body</h2>

<p>PolyU (UGC)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="INS," /><category term="urban" /><category term="canyons," /><category term="GNSS," /><category term="RTK," /><category term="Autonomous" /><category term="System" /><summary type="html"><![CDATA[PolyU (UGC)]]></summary></entry><entry><title type="html">Maximum Consensus Integration of GNSS and LiDAR for Urban Navigation</title><link href="https://polyu-taslab.github.io/2024/01/01/Maximum_Consensus_Integration_of_GNSS_and_LiDAR_for_Urban_Navigation.html" rel="alternate" type="text/html" title="Maximum Consensus Integration of GNSS and LiDAR for Urban Navigation" /><published>2024-01-01T00:00:00+00:00</published><updated>2024-12-16T11:32:04+00:00</updated><id>https://polyu-taslab.github.io/2024/01/01/Maximum_Consensus_Integration_of_GNSS_and_LiDAR_for_Urban_Navigation</id><content type="html" xml:base="https://polyu-taslab.github.io/2024/01/01/Maximum_Consensus_Integration_of_GNSS_and_LiDAR_for_Urban_Navigation.html"><![CDATA[<p>PolyU (UGC)</p>

<h2 id="funding-body">Funding Body</h2>

<p>PolyU (UGC)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="GNSS," /><category term="LIDAR," /><category term="Sensor" /><category term="fusion" /><summary type="html"><![CDATA[PolyU (UGC)]]></summary></entry><entry><title type="html">Multi-robot Collaborative Operations in Lunar Areas for Regolith Processing (first phase)</title><link href="https://polyu-taslab.github.io/2023/12/01/Multi-robot_Collaborative_Operations_in_Lunar_Areas_for_Regolith_Processing_Project.html" rel="alternate" type="text/html" title="Multi-robot Collaborative Operations in Lunar Areas for Regolith Processing (first phase)" /><published>2023-12-01T00:00:00+00:00</published><updated>2024-12-16T11:32:04+00:00</updated><id>https://polyu-taslab.github.io/2023/12/01/Multi-robot_Collaborative_Operations_in_Lunar_Areas_for_Regolith_Processing_Project</id><content type="html" xml:base="https://polyu-taslab.github.io/2023/12/01/Multi-robot_Collaborative_Operations_in_Lunar_Areas_for_Regolith_Processing_Project.html"><![CDATA[<h2 id="abstract">Abstract</h2>

<p>Research on the lunar surface necessitates significant involvement of Unmanned Autonomous Systems (UASs), including autonomous vehicles equipped with robotic arms for the collection of soil samples from the challenging terrain. Two primary challenges must be overcome in executing such tasks: firstly, ensuring precise control accuracy on the rugged surface, and secondly, addressing the safety concerns associated with the operation of space robotics.</p>

<p>In order to address the challenges encountered during the scientific research task, a multi-robot collaboration system has been proposed. The system leverages multi-robot positioning techniques to ensure precise and accurate positioning. By incorporating Factor Graph Optimization (FGO) in conjunction with multi-sensor aided GNSS positioning, the system aims to enhance the overall positioning accuracy. Our FGO-based RTK/INS/Odometer (RIO) integration successfully mitigates outliers, achieving a Root Mean Square Error (RMSE) of 0.30 meters. This represents a significant improvement of 79.2% and 93.6% compared to RTKLIB-RTK and F9P-RTK, respectively. Within our multi-robot framework, FGO-based RIO-OM plays a crucial role in optimizing the collective state of the entire team. By integrating individual robot position estimates, relative position data, and sensor information into a comprehensive global optimization problem, the system ensures efficient and effective collaboration among the robots.</p>

<p>In completing prototype of current phase of this project,Leader-Follower Formation Algorithm is also applied. The algorithm is a low-cost &amp; common strategy for multi-robot formation. In this approach, one robot is designated as the leader, and the other robots, referred to as followers, adjust their positions and movements based on the leader’s state (position and velocity). Each robot controls its own movement to follow the leader and maintain its relative position and distance. Each robot detects obstacles in its surroundings using sensors (e.g., LiDAR, ultrasonic sensors, etc.) and adjusts its trajectory to avoid collisions while maintaining the formation.</p>

<p>In the upcoming stages of this project, our focus will be on enhancing the prototype, with a particular emphasis on improving multi-robot collaboration. Through the implementation of Positioning and Leader-Follower algorithms, the robots will be able to autonomously navigate to the designated soil container locations by utilizing both global and local path planning strategies. The Leader robot will take charge of guiding the Follower robots, thereby ensuring synchronized task execution and accurate navigation towards the target.</p>

<p>To conclude, the multi-robot system designed for simulating the lunar soil sampling process incorporates automated navigation, soil identification, and collection functionalities. Utilizing onboard cameras, the robots capture real-time images and leverage OpenCV image processing algorithms for identifying soil containers embedded with QR codes. Upon successful QR code recognition, the robot promptly computes the precise position and orientation of the soil container to facilitate accurate targeting. The extracted QR code data is then utilized for precise localization and task assignment.</p>

<h2 id="funding-body">Funding Body</h2>

<p>Research Centre for Deep Space Explorations(RCDSE) (Dec 2023 - Dec 2025)</p>

<h2 id="researcher">Researcher</h2>

<p><a href="https://polyu-taslab.github.io/members/Wen_Weisong.html">Dr. Weisong Wen</a> , Qian Liang</p>

<h2 id="status">Status</h2>

<p>Ongoing</p>

<h2 id="system-framework">System Framework</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping_framework.png" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="demonstration">Demonstration</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/speedy_clipped_ms_AO.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/speed_4_combined_arms.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="achievements">Achievements</h2>]]></content><author><name></name></author><category term="Multi-robot" /><category term="collaboration," /><category term="MPC," /><category term="mapping," /><category term="Leader-Follower" /><category term="Formation" /><category term="Algorithm," /><category term="sensor-fusion," /><category term="Visual" /><category term="odometry," /><category term="LiDAR," /><category term="IMU" /><summary type="html"><![CDATA[Abstract Research on the lunar surface necessitates significant involvement of Unmanned Autonomous Systems (UASs), including autonomous vehicles equipped with robotic arms for the collection of soil samples from the challenging terrain. Two primary challenges must be overcome in executing such tasks: firstly, ensuring precise control accuracy on the rugged surface, and secondly, addressing the safety concerns associated with the operation of space robotics. In order to address the challenges encountered during the scientific research task, a multi-robot collaboration system has been proposed. The system leverages multi-robot positioning techniques to ensure precise and accurate positioning. By incorporating Factor Graph Optimization (FGO) in conjunction with multi-sensor aided GNSS positioning, the system aims to enhance the overall positioning accuracy. Our FGO-based RTK/INS/Odometer (RIO) integration successfully mitigates outliers, achieving a Root Mean Square Error (RMSE) of 0.30 meters. This represents a significant improvement of 79.2% and 93.6% compared to RTKLIB-RTK and F9P-RTK, respectively. Within our multi-robot framework, FGO-based RIO-OM plays a crucial role in optimizing the collective state of the entire team. By integrating individual robot position estimates, relative position data, and sensor information into a comprehensive global optimization problem, the system ensures efficient and effective collaboration among the robots. In completing prototype of current phase of this project,Leader-Follower Formation Algorithm is also applied. The algorithm is a low-cost &amp; common strategy for multi-robot formation. In this approach, one robot is designated as the leader, and the other robots, referred to as followers, adjust their positions and movements based on the leader’s state (position and velocity). Each robot controls its own movement to follow the leader and maintain its relative position and distance. Each robot detects obstacles in its surroundings using sensors (e.g., LiDAR, ultrasonic sensors, etc.) and adjusts its trajectory to avoid collisions while maintaining the formation. In the upcoming stages of this project, our focus will be on enhancing the prototype, with a particular emphasis on improving multi-robot collaboration. Through the implementation of Positioning and Leader-Follower algorithms, the robots will be able to autonomously navigate to the designated soil container locations by utilizing both global and local path planning strategies. The Leader robot will take charge of guiding the Follower robots, thereby ensuring synchronized task execution and accurate navigation towards the target. To conclude, the multi-robot system designed for simulating the lunar soil sampling process incorporates automated navigation, soil identification, and collection functionalities. Utilizing onboard cameras, the robots capture real-time images and leverage OpenCV image processing algorithms for identifying soil containers embedded with QR codes. Upon successful QR code recognition, the robot promptly computes the precise position and orientation of the soil container to facilitate accurate targeting. The extracted QR code data is then utilized for precise localization and task assignment. Funding Body Research Centre for Deep Space Explorations(RCDSE) (Dec 2023 - Dec 2025) Researcher Dr. Weisong Wen , Qian Liang Status Ongoing System Framework Demonstration Achievements]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://polyu-taslab.github.io/images/project/speed_4_combined_arms.gif" /><media:content medium="image" url="https://polyu-taslab.github.io/images/project/speed_4_combined_arms.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Vehicle-infrastructure Collaboration for Connected Unmanned Ground and Aerial Vehicles in Complex Urban Canyons</title><link href="https://polyu-taslab.github.io/2023/10/15/Vehicle-infrastructure_Collaboration_for_Connected_Unmanned_Ground_and_Aerial_Vehicles_in_Complex_Urban_Canyons.html" rel="alternate" type="text/html" title="Vehicle-infrastructure Collaboration for Connected Unmanned Ground and Aerial Vehicles in Complex Urban Canyons" /><published>2023-10-15T00:00:00+00:00</published><updated>2024-12-16T11:32:04+00:00</updated><id>https://polyu-taslab.github.io/2023/10/15/Vehicle-infrastructure_Collaboration_for_Connected_Unmanned_Ground_and_Aerial_Vehicles_in_Complex_Urban_Canyons</id><content type="html" xml:base="https://polyu-taslab.github.io/2023/10/15/Vehicle-infrastructure_Collaboration_for_Connected_Unmanned_Ground_and_Aerial_Vehicles_in_Complex_Urban_Canyons.html"><![CDATA[<p>PolyU (UGC)</p>

<h2 id="funding-body">Funding Body</h2>

<p>PolyU (UGC)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="Unmanned" /><category term="Ground" /><category term="Vehicle," /><category term="Unmanned" /><category term="Aerial" /><category term="Vehicle," /><category term="Cooperation," /><category term="urban" /><category term="canyons" /><summary type="html"><![CDATA[PolyU (UGC)]]></summary></entry><entry><title type="html">Research on high-precision vehicle-mounted GNSS/IMU/Camera fusion positioning technology in complex urban environments based on factor graph</title><link href="https://polyu-taslab.github.io/2023/09/30/Research_on_high-precision_vehicle-mounted_GNSS-IMU-Camera_fusion_positioning_technology_in_complex_urban_environments_based_on_factor_graph.html" rel="alternate" type="text/html" title="Research on high-precision vehicle-mounted GNSS/IMU/Camera fusion positioning technology in complex urban environments based on factor graph" /><published>2023-09-30T00:00:00+00:00</published><updated>2024-12-16T11:32:04+00:00</updated><id>https://polyu-taslab.github.io/2023/09/30/Research_on_high-precision_vehicle-mounted_GNSS-IMU-Camera_fusion_positioning_technology_in_complex_urban_environments_based_on_factor_graph</id><content type="html" xml:base="https://polyu-taslab.github.io/2023/09/30/Research_on_high-precision_vehicle-mounted_GNSS-IMU-Camera_fusion_positioning_technology_in_complex_urban_environments_based_on_factor_graph.html"><![CDATA[<p>Tencent Dadi Tongtu (Beijing) Technology Co., Ltd 騰訊大地通途北京科技有限公司</p>

<h2 id="funding-body">Funding Body</h2>

<p>Tencent Dadi Tongtu (Beijing) Technology Co., Ltd 騰訊大地通途北京科技有限公司</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="Sensor" /><category term="fusion," /><category term="FGO," /><category term="Localization" /><summary type="html"><![CDATA[Tencent Dadi Tongtu (Beijing) Technology Co., Ltd 騰訊大地通途北京科技有限公司]]></summary></entry></feed>
<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="/preview/pr-/feed.xml" rel="self" type="application/atom+xml" /><link href="/preview/pr-/" rel="alternate" type="text/html" /><updated>2025-11-06T06:38:49+00:00</updated><id>/preview/pr-/feed.xml</id><title type="html">TAS LAB</title><subtitle>Welcome to the Trustworthy AI and Autonomous Systems Laboratory at the Polytechnic University of Hong Kong. Our research focuses on the development of trustworthy autonomous systems, including autonomous vehicles, drones, and robots.</subtitle><entry><title type="html">Safety-certified Multi-source Fusion Positioning for Autonomous Vehicles in Complex Scenarios</title><link href="/preview/pr-/2025/02/16/Safety-certified_Multi-source_Fusion_Positioning_for_Autonomous_Vehicles_in_Complex_Scenarios.html" rel="alternate" type="text/html" title="Safety-certified Multi-source Fusion Positioning for Autonomous Vehicles in Complex Scenarios" /><published>2025-02-16T00:00:00+00:00</published><updated>2025-11-06T06:35:42+00:00</updated><id>/preview/pr-/2025/02/16/Safety-certified_Multi-source_Fusion_Positioning_for_Autonomous_Vehicles_in_Complex_Scenarios</id><content type="html" xml:base="/preview/pr-/2025/02/16/Safety-certified_Multi-source_Fusion_Positioning_for_Autonomous_Vehicles_in_Complex_Scenarios.html"><![CDATA[<p>Innovation and Technology Commission</p>

<h2 id="funding-body">Funding Body</h2>

<p>Innovation and Technology Commission</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="Localization," /><category term="Sensor" /><category term="fusion," /><category term="Safety," /><category term="Autonomous" /><category term="Vehicle" /><summary type="html"><![CDATA[Innovation and Technology Commission]]></summary></entry><entry><title type="html">Our Autonomous Platforms</title><link href="/preview/pr-/2025/01/01/Our_Autonomous_Platforms.html" rel="alternate" type="text/html" title="Our Autonomous Platforms" /><published>2025-01-01T00:00:00+00:00</published><updated>2025-11-06T06:35:42+00:00</updated><id>/preview/pr-/2025/01/01/Our_Autonomous_Platforms</id><content type="html" xml:base="/preview/pr-/2025/01/01/Our_Autonomous_Platforms.html"><![CDATA[<p>Demonstration of our Autonomous Driving Vehicles and their onboard sensor platforms.</p>

<h2 id="introduction">Introduction</h2>

<p>An autonomous car, also known as a self-driving vehicle, is a sophisticated mode of transportation that can perceive its environment and navigate without human intervention. These vehicles employ a variety of advanced technologies to achieve safe and efficient driving, making them a significant innovation in modern transportation.</p>

<p>A critical aspect of autonomous vehicles is their ability to sense and localize themselves within their surroundings. This capability is essential for navigating complex environments, avoiding obstacles, and making real-time driving decisions. Accurate sensing and localization allow autonomous cars to interpret data from their surroundings and respond appropriately to dynamic conditions.</p>

<p>The autonomous driving vehicle operates under the comprehensive control of a CANBUS system. The host computer establishes a connection with the MCU, which is equipped with integrated ROS messaging capabilities. This integration allows the system to convert ROS messages into CAN signals, which are then transmitted to the MCU.</p>

<p>This architecture provides us with extensive access to the vehicle’s functionalities. We can not only relay vital velocity information but also manage gear settings, including Drive (D), Park (P), Reverse (R), and Neutral (N). Additionally, the system enables control of various lighting functions, enhancing both safety and operational efficiency. Overall, this setup ensures seamless communication between components, facilitating precise control and monitoring of the vehicle’s performance.</p>
<h2 id="sensor-platform">Sensor Platform</h2>

<p>Currently, our lab has two autonomous vehicles deployed on the PolyU Main Campus and the PolyU-Wuxi Research Institute. Both vehicles are equipped with unique sensors, including LiDAR, cameras, and integrated GNSS/INS, for localization and navigation.</p>

<p>Here is the sensor suite:</p>

<table>
  <thead>
    <tr>
      <th>Sensor Type</th>
      <th>Brand/Model</th>
      <th>Parameters</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>**LiDAR**</td>
      <td>Robosense RS-LiDAR-32</td>
      <td>32 laser channels, 200m range, 360° horizontal FOV, 30° vertical FOV, 10Hz-20Hz scanning frequency</td>
    </tr>
    <tr>
      <td>**Cameras**</td>
      <td>HikRobot Event camera</td>
      <td>1280x720 resolution, 120dB dynamic range, 60fps frame rate, global shutter</td>
    </tr>
    <tr>
      <td>**GNSS/INS**</td>
      <td>CHCNav GNSS/INS</td>
      <td>Dual-frequency GNSS receiver, integrated IMU, centimeter-level accuracy, real-time kinematic (RTK) support</td>
    </tr>
  </tbody>
</table>

<h2 id="adv-demo-video">ADV Demo Video</h2>

<h3 id="testing">Testing</h3>
<div style="max-width: 850px; margin: 0 auto; border-radius: 15px; overflow: hidden;">
  <iframe width="850" height="480" src="https://www.youtube.com/embed/Q0nq1vHeinM" frameborder="0" allowfullscreen=""></iframe>
  <p style="text-align: center; margin-top: 10px;">ADV in PolyU Campus</p>
</div>

<div style="max-width: 850px; margin: 0 auto; border-radius: 15px; overflow: hidden;">
  <iframe width="850" height="480" src="https://youtu.be/xPd1KiWvAK8?si=o-ETk9Tx3xcYCiPe" frameborder="0" allowfullscreen=""></iframe>
  <p style="text-align: center; margin-top: 10px;">ADV in PolyU-Wuxi Research Institute</p>
</div>

<!-- <div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/Vehicle/EgoVehicle.gif" alt="Team Banner" 
       style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;">
  <p style="margin-top: 10px; text-align: center;">Autonomous Driving Simulation with Pedastrian Aviodance </p>
</div> -->
<h3 id="carla-simulation-video">Carla Simulation Video</h3>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/Vehicle/Carla.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
  <p style="margin-top: 10px; text-align: center;">Carla Simulation</p>
</div>

<h3 id="researcher">Researcher</h3>

<p><a href="https://polyu-taslab.github.io/members/Wen_Weisong.html">Dr. Weisong Wen</a>, <a href="https://polyu-taslab.github.io/members/Zhang_Ziqi.html">Mr. Zhang Ziqi</a>, <a href="https://polyu-taslab.github.io/members/Huang_Feng.html">Mr. Huang Feng</a></p>]]></content><author><name></name></author><category term="Autonomous-Driving" /><summary type="html"><![CDATA[Demonstration of our Autonomous Driving Vehicles and their onboard sensor platforms.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/preview/pr-/images/project/Vehicle/ADV.png" /><media:content medium="image" url="/preview/pr-/images/project/Vehicle/ADV.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Reliable UAV Perception and Perching Solutions in Urban Streets</title><link href="/preview/pr-/2024/12/09/Reliable_UAV_Perception_and_Perching_Solutions_in_Urban_Streets.html" rel="alternate" type="text/html" title="Reliable UAV Perception and Perching Solutions in Urban Streets" /><published>2024-12-09T00:00:00+00:00</published><updated>2025-11-06T06:35:42+00:00</updated><id>/preview/pr-/2024/12/09/Reliable_UAV_Perception_and_Perching_Solutions_in_Urban_Streets</id><content type="html" xml:base="/preview/pr-/2024/12/09/Reliable_UAV_Perception_and_Perching_Solutions_in_Urban_Streets.html"><![CDATA[<p>Develop a comprehensive UAV perception and Perching solution, focusing on the integration of smart streetlight poles with a UAV takeoff, landing, and battery exchange platform</p>

<h2 id="abstract">Abstract</h2>

<p>This project aims to design and implement a comprehensive UAV perching and management system, tailored to support
autonomous operations in diverse environments. Central to the system is the development of UAV airports equipped
with wireless charging modules, April Tag-based precise landing platforms, and integrated position and orientation
correction mechanisms. These features enable reliable takeoff, landing, and continuous UAV operation with minimal
human intervention.</p>

<p>The system incorporates a robust power management framework, including a custom-designed power distribution board
for seamless transition between onboard battery and external power during charging. Communication between UAVs
and the airport leverages 4G MQTT protocols, ensuring reliable real-time data exchange. Advanced algorithms
integrate GNSS, Vicon, and April Tag-based localization for high-precision positioning, further enhanced by extended
Kalman filtering for improved dynamic performance.</p>

<p>Rigorous experimental validation confirms the system’s effectiveness, including tests on April Tag-based localization,
power management, and wireless communication under dynamic conditions. This project not only addresses critical
challenges in UAV operation and management but also lays the groundwork for scalable UAV infrastructure, fostering
innovation and efficiency in both urban and remote applications.</p>

<div style="display: flex; justify-content: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/UAV_Perching/Smart_Street_light_Poles_with_UAV_Airports.png" alt="Team Banner" style="width: 60%; height: auto; object-fit: cover; max-width: 500px; margin: 0 10px; border-radius: 15px;" />
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/UAV_Perching/Perching_UAV_Flight.jpg" alt="Perching UAV Flight" style="width: 70%; height: auto; object-fit: cover; max-width: 500px; margin: 0 10px; border-radius: 15px;" />
</div>

<h2 id="funding-body">Funding Body</h2>

<p>POLYU AAE(Capstone Project)</p>

<h2 id="researcher">Researcher</h2>

<p><a href="https://polyu-taslab.github.io/members/Wen_Weisong.html">Dr. Weisong Wen</a>, <a href="https://polyu-taslab.github.io/members/ZHAO_Jiaqi.html">Mr. ZHAO Jiaqi</a>, Mr. Li Mingjue, <a href="https://polyu-taslab.github.io/members/hujiahao.html">Mr. HU Jiahao</a>, <a href="https://polyu-taslab.github.io/members/Xiao_Naigui.html">Mr. Xiao Naigui</a>, Mr. FU Chenlei, Mr. Yangmokui</p>

<h2 id="status">Status</h2>

<div style="display: flex; justify-content: center; gap: 20px; margin-bottom: 20px;">
  <div style="text-align: center; margin: 0 10px; display: flex; flex-direction: column; align-items: center;">
    <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/UAV_Perching/compressed_landing_successful.gif" alt="Team Banner" style="width: 60%; height: auto; object-fit: cover; max-width: 600px; border-radius: 8px;" />
    <p style="margin-top: 10px; text-align: center;">UAV Landing Demo</p>
  </div>
  <div style="text-align: center; margin: 0 10px; display: flex; flex-direction: column; align-items: center;">
    <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/UAV_Perching/UAV_Airport.png" alt="Team Banner" style="width: 110%; height: auto; object-fit: cover; max-width: 600px; border-radius: 8px;" />
    <p style="margin-top: 10px; text-align: center;">UAV Airport</p>
  </div>
  <div style="text-align: center; margin: 0 10px; display: flex; flex-direction: column; align-items: center;">
    <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/UAV_Perching/Power_Distribution_Board.png" alt="Team Banner" style="width: 93%; height: auto; object-fit: cover; max-width: 600px; border-radius: 8px;" />
    <p style="margin-top: 10px; text-align: center;">UAV Battery Wireless Charging and Health<br />Management Solution</p>
  </div>
</div>]]></content><author><name></name></author><category term="Landing," /><category term="UAV" /><category term="Perching," /><category term="UAV" /><category term="Airport" /><summary type="html"><![CDATA[Develop a comprehensive UAV perception and Perching solution, focusing on the integration of smart streetlight poles with a UAV takeoff, landing, and battery exchange platform]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/preview/pr-/images/project/UAV_Perching/Smart_Street_light_Poles_with_UAV_Airports.png" /><media:content medium="image" url="/preview/pr-/images/project/UAV_Perching/Smart_Street_light_Poles_with_UAV_Airports.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">AI assisted inertial navigation system</title><link href="/preview/pr-/2024/10/14/AI_assisted_inertial_navigation_system.html" rel="alternate" type="text/html" title="AI assisted inertial navigation system" /><published>2024-10-14T00:00:00+00:00</published><updated>2025-11-06T06:35:42+00:00</updated><id>/preview/pr-/2024/10/14/AI_assisted_inertial_navigation_system</id><content type="html" xml:base="/preview/pr-/2024/10/14/AI_assisted_inertial_navigation_system.html"><![CDATA[<p>This project aims to develop a deep learning-based inertial navigation algorithm that utilizes accelerometer, gyroscope, and magnetometer data from smart wearables and smartphones to infer the user’s position and movement trajectory, while providing corresponding confidence levels</p>

<h2 id="introduction">Introduction</h2>
<p>Inertial odometry is a critical technology used in various applications, from robotics and autonomous vehicles to augmented reality (AR) and wearable devices. It involves estimating the position and orientation of an object over time using data from inertial measurement units (IMUs), which typically include accelerometers and gyroscopes. However, traditional inertial odometry systems often face challenges such as sensor noise, bias, and drift, which can lead to cumulative errors and reduced accuracy over time. To address these challenges, AI-aided inertial odometry has emerged as a promising solution, leveraging the power of artificial intelligence to enhance the performance and reliability of inertial navigation systems. By integrating AI techniques such as machine learning and sensor fusion, these systems can intelligently process and interpret IMU data, correcting for errors and improving overall accuracy. AI-aided inertial odometry systems can learn from patterns in sensor data, adapt to different environments, and integrate information from multiple sources, such as cameras and GPS, to provide more robust and precise motion tracking. This advancement not only mitigates the limitations of traditional inertial systems but also opens up new possibilities for applications in complex and dynamic environments where traditional methods may fall short. As AI continues to evolve, its integration with inertial odometry is expected to drive significant innovations across various fields, enhancing the capabilities of autonomous systems and enriching user experiences in wearable devices. <strong>This project aims to develop a deep learning-based inertial navigation algorithm that utilizes accelerometer, gyroscope, and magnetometer data from smart wearables and smartphones to infer the user’s position and movement trajectory, while providing corresponding confidence levels</strong>.</p>

<h2 id="funding-body">Funding Body</h2>
<p>Honor 上海榮耀智慧科技開發有限公司 (Collaborative Research)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/Honorr.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="Inertial" /><category term="Navigation" /><category term="System," /><category term="AI" /><summary type="html"><![CDATA[This project aims to develop a deep learning-based inertial navigation algorithm that utilizes accelerometer, gyroscope, and magnetometer data from smart wearables and smartphones to infer the user’s position and movement trajectory, while providing corresponding confidence levels]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/preview/pr-/images/project/Honor.png" /><media:content medium="image" url="/preview/pr-/images/project/Honor.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Development of an Assisted Navigation and Collision Avoidance System using AI and Location-based Service</title><link href="/preview/pr-/2024/04/08/Development_of_an_Assisted_Navigation_and_Collision_Avoidance_System_using_AI_and_Location-based_Service.html" rel="alternate" type="text/html" title="Development of an Assisted Navigation and Collision Avoidance System using AI and Location-based Service" /><published>2024-04-08T00:00:00+00:00</published><updated>2025-11-06T06:35:42+00:00</updated><id>/preview/pr-/2024/04/08/Development_of_an_Assisted_Navigation_and_Collision_Avoidance_System_using_AI_and_Location-based_Service</id><content type="html" xml:base="/preview/pr-/2024/04/08/Development_of_an_Assisted_Navigation_and_Collision_Avoidance_System_using_AI_and_Location-based_Service.html"><![CDATA[<h2 id="abstract">Abstract</h2>

<p>Urban roadways often face challenges such as building-induced obstructions, creating significant blind spots and positioning inaccuracies that hinder safe lane spacing and inter-vehicle distance management. Existing solutions lack cost-effective enhancements for positioning and over-the-horizon collaborative sensing technologies, particularly in dense urban environments.</p>

<p>To address these issues, this project proposes a low-cost, high-precision co-location solution specifically designed for urban canyons. Additionally, a collision avoidance warning application is developed to provide early warnings and emergency interventions for collision risks in blind spots.</p>

<p>The proposed approach integrates advanced methodologies: (1) Lane-level positioning services utilizing Real-Time Kinematic (RTK) and 3D city maps for precise lane-level positioning, (2) Multi-vehicle collaborative sensing leveraging artificial intelligence (AI) and RTK to enable over-the-horizon sensing and early warnings, and (3) Dynamic model optimization using AI to refine positioning accuracy and develop a prototype multi-vehicle collision avoidance system.</p>

<p>The solution achieves lane-level positioning accuracy with an error margin of less than 0.5 meters and provides collision risk warnings at varying inter-vehicle distances. By significantly reducing traffic accidents caused by urban road challenges, this project offers a transformative step toward safer and more reliable urban transportation systems.</p>

<h2 id="funding-body">Funding Body</h2>

<p>Smart Traffic Fund</p>

<h2 id="researcher">Researcher</h2>

<p><a href="https://polyu-taslab.github.io/members/Wen_Weisong.html">Dr. Weisong Wen</a>, Dr. Li-Ta Hsu, Dr. Guohao Zhang, Dr. Jian Liu, <a href="https://polyu-taslab.github.io/members/Huang_Feng.html">Feng Huang</a>, <a href="https://polyu-taslab.github.io/members/liu_xikun.html">Xikun Liu</a>， <a href="https://polyu-taslab.github.io/members/yihan_zhong.html">Yihan Zhong</a>， <a href="https://polyu-taslab.github.io/members/wang_xin.html">Xin Wang</a>, Yuan Li， <a href="https://polyu-taslab.github.io/members/runzhi_hu.html">Runzhi Hu</a></p>

<h2 id="status">Status</h2>

<p>Completed</p>

<h2 id="system-framework">System Framework</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/stf/pipeline.png" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="demonstration">Demonstration</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/stf/demo_gif.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="achievements">Achievements</h2>

<ul>
  <li>Li, Y., Liu, X., Wen, W., Hsu, L. T., Yuan, Y., Bian, G., &amp; Chen, Q. (2024, September). Factor Graph Optimization Based Multi Epoch Ambiguity Resolution for GNSS RTK and its Evaluation in Hong Kong Urban Canyons. In Proceedings of the 37th International Technical Meeting of the Satellite Division of The Institute of Navigation (ION GNSS+ 2024) (pp. 2151-2162).</li>
</ul>]]></content><author><name></name></author><category term="Positioning" /><category term="Services," /><category term="Multi-Vehicle" /><category term="Collaborative" /><category term="Sensing," /><category term="AI" /><category term="aided" /><category term="GNSS," /><category term="GNSS" /><category term="Signal" /><category term="Tracing," /><category term="Sensor" /><category term="Integration" /><summary type="html"><![CDATA[Abstract]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/preview/pr-/images/project/stf/demo_gif.gif" /><media:content medium="image" url="/preview/pr-/images/project/stf/demo_gif.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Safe-assured Learning-based Deep SE(3) Motion Joint Planning and Control for Unmanned Aerial Vehicles</title><link href="/preview/pr-/2024/04/08/Safe-assured_Learning-based_Deep_SE(3)_Motion_Joint_Planning_and_Control_for_Unmanned_Aerial_Vehicles.html" rel="alternate" type="text/html" title="Safe-assured Learning-based Deep SE(3) Motion Joint Planning and Control for Unmanned Aerial Vehicles" /><published>2024-04-08T00:00:00+00:00</published><updated>2025-11-06T06:35:42+00:00</updated><id>/preview/pr-/2024/04/08/Safe-assured_Learning-based_Deep_SE(3)_Motion_Joint_Planning_and_Control_for_Unmanned_Aerial_Vehicles</id><content type="html" xml:base="/preview/pr-/2024/04/08/Safe-assured_Learning-based_Deep_SE(3)_Motion_Joint_Planning_and_Control_for_Unmanned_Aerial_Vehicles.html"><![CDATA[<p>PolyU (UGC)</p>

<h2 id="funding-body">Funding Body</h2>

<p>PolyU (UGC)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="Advanced" /><category term="Vehicle" /><category term="Safety" /><category term="Systems," /><category term="Automated" /><category term="Vehicle" /><category term="Operation," /><category term="Motion" /><category term="Planning," /><category term="Navigation," /><category term="Aerial," /><category term="Marine" /><category term="and" /><category term="Surface" /><category term="Intelligent" /><category term="Vehicles" /><summary type="html"><![CDATA[PolyU (UGC)]]></summary></entry><entry><title type="html">Sustainable Window Cleaning for PolyU Jockey Club Innovation Tower with Unmanned Aerial Vehicles (UAV):An Application of Autonomous Systems Enabled Carbon Reduction</title><link href="/preview/pr-/2024/04/01/Sustainable_Window_Cleaning_for_PolyU_Jockey_Club_Innovation_Tower_with_Unmanned_Aerial_Vehicles-(UAV)_An_Application_of_Autonomous_Systems_Enabled_Carbon_Reduction.html" rel="alternate" type="text/html" title="Sustainable Window Cleaning for PolyU Jockey Club Innovation Tower with Unmanned Aerial Vehicles (UAV):An Application of Autonomous Systems Enabled Carbon Reduction" /><published>2024-04-01T00:00:00+00:00</published><updated>2025-11-06T06:35:42+00:00</updated><id>/preview/pr-/2024/04/01/Sustainable_Window_Cleaning_for_PolyU_Jockey_Club_Innovation_Tower_with_Unmanned_Aerial_Vehicles%20(UAV)_An_Application_of_Autonomous_Systems_Enabled_Carbon_Reduction</id><content type="html" xml:base="/preview/pr-/2024/04/01/Sustainable_Window_Cleaning_for_PolyU_Jockey_Club_Innovation_Tower_with_Unmanned_Aerial_Vehicles-(UAV)_An_Application_of_Autonomous_Systems_Enabled_Carbon_Reduction.html"><![CDATA[<p>PolyU (UGC)</p>

<h2 id="introduction">Introduction</h2>
<p>Skyscrapers are one of the features of Hong Kong and its window cleaning is a time-consuming and dangerous work that needs to be completed regularly. The typical solution is to use big machines, such as the “Movable platform” or the “Bosun’s chair” with high power, to navigate the window cleaners to the building’s exterior. Take the PolyU Jockey Club Innovation Tower (JCIT) as an example, 20,000 HKD are required by the cleaning industry to clean 100 m^2 areas (less than 1/20 of the total areas of the JCIT surface) in two weeks. Based on the fuel assumption, a value of carbon emission of 277.8 kgCO2 was caused by the JCIT cleaning in two weeks. The value of the carbon emission will be even more significant when cleaning the rest of the JCIT due to the irregular shape of the JCIT. In fact, part of the JCIT surface is very difficult to clean due to the highly irregular surface which is out of the reach of the typical “Movable platform” or the “Bosun’s chair”. Bigger machines are required but lead to unacceptable carbon reduction accordingly. A solution to efficiently complete the window cleaning is practical and highly expected, which can directly help with carbon reduction at the PolyU campus. Moreover, this is highly expected by lots of “Skyscrapers” in Hong Kong.</p>

<p>To fill this gap, this project proposes to use unmanned aerial vehicles (UAV) to complete the building window cleaning task efficiently after several meeting discussions between the project team members and the PolyU Campus Facilities and Sustainability Office (CFSO), aiming at reducing the carbon emission from existing solutions. The project team has previously accumulated significant achievements in terms of UAV technologies, such as navigation, control, and planning, which is supported by the team’s academic publications records and knowledge transfer to the UAV industry, such as the Meituan with applications for parcel delivery. Also, the project team has the capability of designing the UAV system from scratch. Based on the prototype of the UAV for building window cleaning, the value of carbon emission would be reduced from 277.8 kgCO2 using the existing solution by the cleaning industry, to around 2.91 kgCO2 using the UAV system. The achievement in carbon reduction can be even more significant upon the success and completion of this project. We fully believe that this project will introduce significant impacts to the carbon reduction plan of PolyU, and will also lead to potential impacts to the carbon reduction of Hong Kong.</p>

<h2 id="funding-body">Funding Body</h2>
<p>Carbon Neutrality Funding Scheme 2023/24 (PolyU)</p>

<h2 id="researcher">Researcher</h2>
<p><a href="https://polyu-taslab.github.io/members/Wen_Weisong.html">Dr. Weisong Wen</a></p>

<h2 id="status">Status</h2>
<p>Ongoing</p>

<h2 id="technology--method">Technology &amp; Method</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/green_tech.png" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="potential-contributions-to-carbon-reduction">Potential Contributions to Carbon Reduction</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/carbon_reduce.png" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="uav-test">UAV Test</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <h1 style="font-size: 1.5em; margin-bottom: 20px; line-height: 1.2;">
    Outdoor test
  </h1>
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/windows_cleaning_test.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<div style="text-align: center; margin-bottom: 20px;">
  <h1 style="font-size: 1.5em; margin-bottom: 20px; line-height: 1.2;">
    Indoor test
  </h1>
  <iframe width="560" height="315" src="https://www.youtube.com/embed/m2Lm8RY2uYI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</div>

<h2 id="interview">Interview</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <h1 style="font-size: 1.5em; margin-bottom: 20px; line-height: 1.2;">
    TVB interviewed us about the low-altitude economy (LAE)
  </h1>
  <img width="560" height="315" src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/TVB.JPG" alt="TVB Interview about LAE" />
  <iframe width="560" height="315" src="//player.bilibili.com/player.html?bvid=BV1bhrKY3EcJ&amp;spm_id_from=333.999.0.0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe>
</div>

<p style="text-align: left; font-size: 1.0em; margin-bottom: 10px;">
  Video link:
</p>
<div style="text-align: left; font-size: 1.0em;">
  <a href="https://evtolasia.com/2024/06/01/hong-kongs-low-altitude-economy-opportunities-and-policy-considerations" target="_blank" style="display: block; color: #007BFF; text-decoration: none;">
    https://evtolasia.com/2024/06/01/hong-kongs-low-altitude-economy-opportunities-and-policy-considerations
  </a>
  <a href="https://news.tvb.com/tc/programme/financemagazine" target="_blank" style="display: block; color: #007BFF; text-decoration: none;">
    https://news.tvb.com/tc/programme/financemagazine
  </a>
  <a href="https://news.tvb.com/tc/finance/665a72c4b08fa1987b3d6350/財經-財經透視香港低空經濟何時起飛" target="_blank" style="display: block; color: #007BFF; text-decoration: none;">
    https://news.tvb.com/tc/finance/665a72c4b08fa1987b3d6350/財經-財經透視香港低空經濟何時起飛
  </a>
</div>

<div style="text-align: center; margin-bottom: 20px;">
  <h1 style="font-size: 1.5em; margin-bottom: 20px; line-height: 1.2;">
    Mingpao interviewed us about the low-altitude economy (LAE)
  </h1>
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/MINGBAO.JPG" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<p style="text-align: left; font-size: 1.0em; margin-bottom: 10px;">
  Video link:
</p>
<div style="text-align: left; font-size: 1.0em;">
  <a href="https://news.mingpao.com/pns/港聞/article/20240531/s00002/1717094795920/學者倡訂法規認證基建助低空經濟-研無人機洗大廈外牆-料潛在利潤每年逾3億" target="_blank" style="display: block; color: #007BFF; text-decoration: none;">
    https://news.mingpao.com/pns/港聞/article/20240531/s00002/1717094795920/學者倡訂法規認證基建助低空經濟-研無人機洗大廈外牆-料潛在利潤每年逾3億
  </a>
  <a href="https://www.youtube.com/watch?v=m2Lm8RY2uYI" target="_blank" style="display: block; color: #007BFF; text-decoration: none;">
    https://www.youtube.com/watch?v=m2Lm8RY2uYI
  </a>
</div>

<div style="text-align: center; margin-bottom: 20px;">
  <h1 style="font-size: 1.5em; margin-bottom: 20px; line-height: 1.2;">
    Interview with RTHK on the Development of Drones in the Low Altitude Economy (LAE)
  </h1>
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/rthk.jpg" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<p style="text-align: left; font-size: 1.0em; margin-bottom: 10px;">
  Video link:
</p>
<div style="text-align: left; font-size: 1.0em;">
  <a href="https://www.rthk.hk/tv/dtt32/programme/vibranthongkong/episode/959611" target="_blank" style="display: block; color: #007BFF; text-decoration: none;">
    https://www.rthk.hk/tv/dtt32/programme/vibranthongkong/episode/959611
  </a>
</div>]]></content><author><name></name></author><category term="Unmanned" /><category term="Aerial" /><category term="Vehicle" /><summary type="html"><![CDATA[PolyU (UGC)]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/preview/pr-/images/project/uav_clean.png" /><media:content medium="image" url="/preview/pr-/images/project/uav_clean.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Data-driven-assisted GNSS RTK/INS Navigation for Autonomous Systems in Urban Canyons </title><link href="/preview/pr-/2024/01/01/Data-driven-assisted_GNSS_RTK-INS_Navigation_for_Autonomous_Systems_in_Urban_Canyons.html" rel="alternate" type="text/html" title="Data-driven-assisted GNSS RTK/INS Navigation for Autonomous Systems in Urban Canyons " /><published>2024-01-01T00:00:00+00:00</published><updated>2025-11-06T06:35:42+00:00</updated><id>/preview/pr-/2024/01/01/Data-driven-assisted_GNSS_RTK-INS_Navigation_for_Autonomous_Systems_in_Urban_Canyons</id><content type="html" xml:base="/preview/pr-/2024/01/01/Data-driven-assisted_GNSS_RTK-INS_Navigation_for_Autonomous_Systems_in_Urban_Canyons.html"><![CDATA[<h2 id="abstract">Abstract</h2>

<p>The great development of smart urban transportation has led to the booming of the autonomous vehicle industry. Society places great expectations on autonomous driving to enhance safety and efficiency in transportation systems, particularly in challenging environments like urban canyons where precise positioning is crucial. Global Navigation Satellite System (GNSS) positioning modules are the crucial element of autonomous vehicles for absolute positioning. However, the GNSS pseudorange suffers significant biases and degradation due to the multipath and non-line-of-sight (NLOS) errors caused by tall buildings that widely exist in urban canyons. To improve GNSS positioning accuracy in harsh urban canyons, this paper introduces a deep multimodal learning-based approach to correct distorted pseudorange measurements. The vision features of the environment captured by the camera are integrated to form the multimodal network with GNSS measurements. Meanwhile, positional encoding (PE) is proposed to fuse the image features and the satellite position information in higher-dimensional space. The biases are outputted by the network and are used for improved positioning. Compared to existing solutions, extensive test results show that the proposed method achieves improved positioning accuracy ranging from 12\% to 20\%. Additionally, the method proposed in this paper takes only 0.5 seconds to perform epoch-by-epoch pseudorange correction on Jetson Orin Nano, which meets the real-time requirements and shows extremely high practical value. This paper also opensources a dataset under varying time, satellite distributions, and lighting conditions with accurate NLOS labels. We aim to contribute this dataset to the GNSS and deep learning communities, aspiring to establish it as the benchmark for NLOS classification and pseudorange bias correction. The datasets can be accessed at <a href="https://github.com/ebhrz/KLTDataset">github</a>.</p>

<h2 id="funding-body">Funding Body</h2>

<p>PolyU (UGC)</p>

<h2 id="researcher">Researcher</h2>

<p><a href="https://polyu-taslab.github.io/members/Wen_Weisong.html">Dr. Weisong Wen</a>, <a href="https://polyu-taslab.github.io/members/runzhi_hu.html">Runzhi Hu</a>, Dr. Jian Liu, <a href="https://polyu-taslab.github.io/members/yihan_zhong.html">Yihan Zhong</a>, Dr. Ming Xia</p>

<h2 id="status">Status</h2>

<p>Undergoing</p>

<h2 id="system-framework">System Framework</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/Data_driven_structure.png" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="positioning-results">Positioning Results</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/Data_driven_boxplot.png" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="achievements">Achievements</h2>

<ul>
  <li>Dataset Link: <a href="https://github.com/ebhrz/KLTDataset">https://github.com/ebhrz/KLTDataset</a></li>
</ul>]]></content><author><name></name></author><category term="GNSS" /><category term="Positioning," /><category term="NLOS/Multipath" /><category term="Correction," /><category term="Autonomous" /><category term="Driving," /><category term="Positional" /><category term="Encoding," /><category term="Multimodal" /><category term="Network," /><category term="Vision" /><category term="Feature" /><summary type="html"><![CDATA[Abstract]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/preview/pr-/images/project/Data_driven_structure.png" /><media:content medium="image" url="/preview/pr-/images/project/Data_driven_structure.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Maximum Consensus Integration of GNSS and LiDAR for Urban Navigation</title><link href="/preview/pr-/2024/01/01/Maximum_Consensus_Integration_of_GNSS_and_LiDAR_for_Urban_Navigation.html" rel="alternate" type="text/html" title="Maximum Consensus Integration of GNSS and LiDAR for Urban Navigation" /><published>2024-01-01T00:00:00+00:00</published><updated>2025-11-06T06:35:42+00:00</updated><id>/preview/pr-/2024/01/01/Maximum_Consensus_Integration_of_GNSS_and_LiDAR_for_Urban_Navigation</id><content type="html" xml:base="/preview/pr-/2024/01/01/Maximum_Consensus_Integration_of_GNSS_and_LiDAR_for_Urban_Navigation.html"><![CDATA[<p>PolyU (UGC)</p>

<h2 id="funding-body">Funding Body</h2>

<p>PolyU (UGC)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="GNSS," /><category term="LIDAR," /><category term="Sensor" /><category term="fusion" /><summary type="html"><![CDATA[PolyU (UGC)]]></summary></entry><entry><title type="html">Multi-robot Collaborative Operations in Lunar Areas for Regolith Processing</title><link href="/preview/pr-/2023/12/03/Multi_robot_Collaborative_Operations_in_Lunar_Areas_for_Regolith_Processing_Project.html" rel="alternate" type="text/html" title="Multi-robot Collaborative Operations in Lunar Areas for Regolith Processing" /><published>2023-12-03T00:00:00+00:00</published><updated>2025-11-06T06:35:42+00:00</updated><id>/preview/pr-/2023/12/03/Multi_robot_Collaborative_Operations_in_Lunar_Areas_for_Regolith_Processing_Project</id><content type="html" xml:base="/preview/pr-/2023/12/03/Multi_robot_Collaborative_Operations_in_Lunar_Areas_for_Regolith_Processing_Project.html"><![CDATA[<h2 id="abstract">Abstract</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/rugged_surface_problem_lunar.jpg" alt="Team Banner" style="width: 50%; height: auto; object-fit: cover; max-width: 350px; margin: 0 auto; border-radius: 15px;" />
</div>

<p>Research conducted on the lunar surface necessitates the utilization of Unmanned Autonomous Systems (UASs) equipped with sophisticated robotic arms designed for the purpose of collecting soil samples. The primary challenges encountered in this endeavor revolve around the need for precise control over the UASs while navigating through rugged and uneven terrain, all while ensuring the utmost safety of the operation. To tackle these challenges head-on, a cutting-edge multi-robot collaboration system has been conceptualized and put forth as a solution. This innovative system leverages the power of Factor Graph Optimization (FGO) to achieve unparalleled accuracy in positioning the UASs during the soil sample collection process.</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/positioning error.jpg" alt="Team Banner" style="width: 50%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<p>Remarkably, the implementation of this collaborative system has yielded remarkable results, boasting a Root Mean Square Error (RMSE) of merely 0.30 meters. This achievement represents a substantial advancement compared to the conventional methods previously employed in similar research endeavors. A pivotal component of this system is the integration of the Leader-Follower Formation Algorithm, which plays a crucial role in facilitating efficient coordination among multiple robots operating simultaneously in the lunar environment.</p>

<p>Looking ahead, the future research trajectory will be centered around further enhancing the collaborative capabilities of the system. This will entail a deep dive into refining the existing Positioning and Leader-Follower algorithms, with a specific focus on optimizing autonomous navigation techniques and streamlining the soil sampling process. Noteworthy features of the system include automated navigation functionalities, advanced soil identification mechanisms, and seamless soil collection procedures facilitated by the utilization of QR codes for precise localization and task allocation purposes.</p>

<h2 id="funding-body">Funding Body</h2>

<p>Research Centre for Deep Space Explorations  (Dec 2023 - Dec 2025)</p>

<h2 id="researcher">Researcher</h2>

<p><a href="https://polyu-taslab.github.io/members/Wen_Weisong.html">Dr. Weisong Wen</a>, Qian Liang</p>

<h2 id="status">Status</h2>

<p>Ongoing</p>

<h2 id="system-framework">System Framework</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/prototype.png" alt="Team Banner" style="width: 60%; height: auto; object-fit: cover; max-width: 320px; margin: 0 auto; border-radius: 15px;" />
</div>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/overall_architecture_lunar.jpg" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="mapping-results">Mapping Results</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/speed_4_combined_arms.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/speedy_clipped_ms_AO.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="achievements">Achievements</h2>]]></content><author><name></name></author><category term="Multi-robot-collaboration," /><category term="MPC," /><category term="mapping," /><category term="Leader-Follower-Formation-Algorithm," /><category term="sensor-fusion," /><category term="LiDAR," /><category term="IMU" /><summary type="html"><![CDATA[Abstract]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/preview/pr-/images/project/prototype.png" /><media:content medium="image" url="/preview/pr-/images/project/prototype.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>
<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="/preview/pr-/feed.xml" rel="self" type="application/atom+xml" /><link href="/preview/pr-/" rel="alternate" type="text/html" /><updated>2026-02-15T05:31:03+00:00</updated><id>/preview/pr-/feed.xml</id><title type="html">TAS LAB</title><subtitle>Welcome to the Trustworthy AI and Autonomous Systems Laboratory at the Polytechnic University of Hong Kong. Our research focuses on the development of trustworthy autonomous systems, including autonomous vehicles, drones, and robots.</subtitle><entry><title type="html">Safety-certified Multi-source Fusion Positioning for Autonomous Vehicles in Complex Scenarios</title><link href="/preview/pr-/2025/02/16/Safety-certified_Multi-source_Fusion_Positioning_for_Autonomous_Vehicles_in_Complex_Scenarios.html" rel="alternate" type="text/html" title="Safety-certified Multi-source Fusion Positioning for Autonomous Vehicles in Complex Scenarios" /><published>2025-02-16T00:00:00+00:00</published><updated>2026-02-15T05:28:27+00:00</updated><id>/preview/pr-/2025/02/16/Safety-certified_Multi-source_Fusion_Positioning_for_Autonomous_Vehicles_in_Complex_Scenarios</id><content type="html" xml:base="/preview/pr-/2025/02/16/Safety-certified_Multi-source_Fusion_Positioning_for_Autonomous_Vehicles_in_Complex_Scenarios.html"><![CDATA[<p>Innovation and Technology Commission</p>

<h2 id="funding-body">Funding Body</h2>

<p>Innovation and Technology Commission</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="Localization," /><category term="Sensor" /><category term="fusion," /><category term="Safety," /><category term="Autonomous" /><category term="Vehicle" /><summary type="html"><![CDATA[Innovation and Technology Commission]]></summary></entry><entry><title type="html">Our Autonomous Platforms</title><link href="/preview/pr-/2025/01/01/Our_Autonomous_Platforms.html" rel="alternate" type="text/html" title="Our Autonomous Platforms" /><published>2025-01-01T00:00:00+00:00</published><updated>2026-02-15T05:28:27+00:00</updated><id>/preview/pr-/2025/01/01/Our_Autonomous_Platforms</id><content type="html" xml:base="/preview/pr-/2025/01/01/Our_Autonomous_Platforms.html"><![CDATA[<p>Our cutting-edge research platforms for end-to-end AI self-driving, where neural networks learn to drive directly from sensor data to control outputs.</p>

<h2 id="what-is-end-to-end-ai-self-driving">What is End-to-End AI Self-Driving?</h2>

<p>End-to-end AI self-driving represents a paradigm shift in autonomous vehicle technology. Unlike traditional modular pipelines that break down driving into separate perception, prediction, planning, and control modules, end-to-end approaches use deep neural networks to learn the entire driving task holistically—directly mapping raw sensor inputs to vehicle control commands.</p>

<p>This revolutionary approach offers several key advantages:</p>

<p><strong>Direct Sensor-to-Control Learning</strong>: Neural networks process multi-modal sensor data (cameras, LiDAR, GNSS) and output steering angles, throttle, and brake commands in a single forward pass, eliminating the error propagation inherent in modular systems.</p>

<p><strong>Learned Representations</strong>: Rather than hand-crafting features and rules, the network automatically discovers optimal internal representations of the driving environment, capturing subtle patterns that human engineers might miss.</p>

<p><strong>Data-Driven Adaptation</strong>: End-to-end models continuously improve through exposure to diverse driving scenarios, learning complex behaviors like defensive driving, traffic flow prediction, and context-aware decision-making from demonstration data.</p>

<p><strong>Unified Optimization</strong>: The entire driving pipeline is optimized jointly using gradient-based learning, ensuring that perception and control work synergistically rather than as isolated components.</p>

<p>Our research explores multiple end-to-end architectures—from imitation learning systems that mimic expert drivers to reinforcement learning agents that discover optimal policies through trial and error in simulation, then transfer to real-world deployment.</p>

<h2 id="introduction">Introduction</h2>

<p>Autonomous vehicles represent the future of intelligent transportation, leveraging end-to-end AI architectures to transform raw sensor data into safe, human-like driving decisions. Our laboratory develops and deploys advanced self-driving systems that embody the latest breakthroughs in deep learning, computer vision, and robotics.</p>

<p>At the core of our autonomous platforms is an integrated AI pipeline that processes multi-modal sensor streams—LiDAR point clouds, camera images, and GNSS/INS data—through sophisticated neural network architectures. These systems learn to simultaneously perceive the environment, predict future trajectories, and execute driving maneuvers in real-time, handling complex urban scenarios with human-level performance.</p>

<p>The autonomous driving vehicle operates under comprehensive CANBUS control integrated with ROS2 middleware. Our AI control stack communicates seamlessly with the vehicle’s MCU, translating high-level neural network outputs into low-level CAN signals for precise actuation. This architecture enables full drive-by-wire control including:</p>

<ul>
  <li><strong>Longitudinal control</strong>: Acceleration and braking commands derived from learned policies</li>
  <li><strong>Lateral control</strong>: Steering angles predicted by end-to-end neural networks</li>
  <li><strong>Mode management</strong>: Automated gear shifting (D/P/R/N) based on mission planning</li>
  <li><strong>Safety systems</strong>: AI-monitored lighting, indicators, and fail-safe mechanisms</li>
</ul>

<p>This platform serves as our testbed for advancing AI-powered autonomous driving, from imitation learning and reinforcement learning to vision-language models for natural language navigation.</p>

<h2 id="end-to-end-ai-architecture-components">End-to-End AI Architecture Components</h2>

<p>Our autonomous driving system implements a comprehensive end-to-end AI architecture comprising the following key components:</p>

<h3 id="1-multi-modal-perception-network">1. Multi-Modal Perception Network</h3>
<p><strong>Function</strong>: Fuses data from cameras, LiDAR, and GNSS/INS into unified spatial-temporal representations</p>

<p><strong>Architecture</strong>: Vision backbone (ResNet, EfficientNet, or Vision Transformers) for image feature extraction; PointNet++/VoxelNet for 3D point cloud processing; Multi-scale feature pyramid networks for detecting objects at various distances; Temporal fusion modules (ConvLSTM, 3D CNNs) for motion prediction</p>

<p><strong>Outputs</strong>: Bird’s-eye-view (BEV) semantic maps, 3D object detections, drivable area segmentation, lane boundary predictions</p>

<h3 id="2-world-model--prediction">2. World Model &amp; Prediction</h3>
<p><strong>Function</strong>: Learns predictive models of how the environment evolves over time</p>

<p><strong>Architecture</strong>: Recurrent neural networks (GRU/LSTM) or Transformers for sequential prediction; Probabilistic trajectory forecasting for surrounding vehicles and pedestrians; Occupancy grid prediction for future scene states; Attention mechanisms for modeling agent-agent interactions</p>

<p><strong>Outputs</strong>: Multi-modal future trajectory distributions, predicted collision risks, uncertainty estimates</p>

<h3 id="3-planning--decision-making-network">3. Planning &amp; Decision-Making Network</h3>
<p><strong>Function</strong>: Generates safe, comfortable, and efficient driving trajectories</p>

<p><strong>Architecture</strong>: Hierarchical planning with high-level route planning and low-level trajectory optimization; Imitation learning from expert demonstrations (Behavioral Cloning, GAIL, DAgger); Reinforcement learning for reward-driven policy optimization (PPO, SAC, TD3); Cost volume networks for evaluating trajectory candidates; Attention-based reasoning for traffic rule compliance</p>

<p><strong>Outputs</strong>: Reference trajectories (waypoints with velocity profiles), discrete actions (lane changes, stops)</p>

<h3 id="4-control-network">4. Control Network</h3>
<p><strong>Function</strong>: Executes planned trajectories through precise vehicle control</p>

<p><strong>Architecture</strong>: PID controllers enhanced with learned gain scheduling; Model Predictive Control (MPC) with learned dynamics models; Direct end-to-end control networks (steering/throttle/brake prediction); Residual learning to compensate for model uncertainties</p>

<p><strong>Outputs</strong>: Low-level commands (steering angle, throttle percentage, brake pressure)</p>

<h3 id="5-safety--verification-layer">5. Safety &amp; Verification Layer</h3>
<p><strong>Function</strong>: Ensures AI decisions meet safety constraints and override when necessary</p>

<p><strong>Components</strong>: Learned safety filters using reachability analysis; Rule-based fallback systems for edge cases; Uncertainty-aware decision-making (epistemic and aleatoric uncertainty); Real-time monitoring and anomaly detection; Redundant sensor validation and fault diagnosis</p>

<p><strong>Outputs</strong>: Safety scores, intervention flags, fail-safe commands</p>

<h3 id="6-continuous-learning-pipeline">6. Continuous Learning Pipeline</h3>
<p><strong>Function</strong>: Enables the system to improve from real-world deployment data</p>

<p><strong>Components</strong>: On-vehicle data logging (sensor streams, AI decisions, interventions); Offline reinforcement learning from logged experience; Active learning for identifying informative scenarios; Sim-to-real transfer learning using domain adaptation; Federated learning across vehicle fleet</p>

<p><strong>Outputs</strong>: Updated model weights, identified edge cases, performance metrics</p>

<h2 id="sensor-platform">Sensor Platform</h2>

<p>Our laboratory operates two autonomous vehicle testbeds—one at PolyU Main Campus and another at PolyU-Wuxi Research Institute—both equipped with production-grade sensor suites for multi-modal AI training and validation.</p>

<p>The sensor configuration enables comprehensive environmental perception:</p>

<table>
  <thead>
    <tr>
      <th>Sensor Type</th>
      <th>Brand/Model</th>
      <th>Specifications</th>
      <th>AI Application</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>**LiDAR**</td>
      <td>Robosense RS-LiDAR-32</td>
      <td>32 channels, 200m range, 360° FOV, 30° vertical FOV, 10-20Hz</td>
      <td>3D point cloud processing for obstacle detection, semantic segmentation, and occupancy prediction</td>
    </tr>
    <tr>
      <td>**Cameras**</td>
      <td>HikRobot Event Camera</td>
      <td>1280×720 resolution, 120dB HDR, 60fps, global shutter</td>
      <td>Vision-based perception, lane detection, traffic sign recognition, end-to-end driving policy learning</td>
    </tr>
    <tr>
      <td>**GNSS/INS**</td>
      <td>CHCNav GNSS/INS</td>
      <td>Dual-frequency RTK, integrated IMU, cm-level accuracy</td>
      <td>Ground-truth localization for supervised learning, map-based planning, sensor fusion validation</td>
    </tr>
  </tbody>
</table>

<p>This sensor fusion architecture provides redundant, complementary data streams that feed our end-to-end AI models, enabling robust perception under diverse weather and lighting conditions.</p>

<h2 id="ai-driven-autonomous-driving-demonstrations">AI-Driven Autonomous Driving Demonstrations</h2>

<h3 id="real-world-testing-campus-deployment">Real-World Testing: Campus Deployment</h3>

<div style="max-width: 850px; margin: 0 auto; border-radius: 15px; overflow: hidden;">
  <iframe width="850" height="480" src="https://www.youtube.com/embed/Q0nq1vHeinM" frameborder="0" allowfullscreen=""></iframe>
  <p style="text-align: center; margin-top: 10px;"><strong>End-to-End AI Navigation</strong> — PolyU Campus</p>
</div>

<div style="max-width: 850px; margin: 0 auto; border-radius: 15px; overflow: hidden;">
  <iframe width="850" height="480" src="https://www.youtube.com/embed/xPd1KiWvAK8" frameborder="0" allowfullscreen=""></iframe>
  <p style="text-align: center; margin-top: 10px;"><strong>Autonomous Operation</strong> — PolyU-Wuxi Research Institute</p>
</div>

<h3 id="ai-training-pipeline-carla-simulation">AI Training Pipeline: CARLA Simulation</h3>

<p>Our AI models are pre-trained and validated in high-fidelity simulation environments before real-world deployment. Using CARLA simulator, we generate diverse driving scenarios for imitation learning, reinforcement learning, and domain adaptation research.</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/Vehicle/Carla.gif" alt="CARLA Simulation" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
  <p style="margin-top: 10px; text-align: center;"><strong>CARLA Simulation Environment</strong> — End-to-End AI Policy Learning</p>
</div>

<!-- Pedestrian avoidance demonstration -->
<!-- <div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/Vehicle/EgoVehicle.gif" alt="Pedestrian Avoidance" 
       style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;">
  <p style="margin-top: 10px; text-align: center;">Autonomous Driving with AI-Based Pedestrian Avoidance</p>
</div> -->

<h2 id="research-team">Research Team</h2>

<p><strong>Principal Investigator:</strong><br />
<a href="https://polyu-taslab.github.io/members/Wen_Weisong.html">Dr. Wen Weisong</a> — Assistant Professor, Department of Aeronautical and Aviation Engineering, The Hong Kong Polytechnic University</p>

<p><strong>Core Researchers:</strong><br />
<a href="https://polyu-taslab.github.io/members/Zhang_Ziqi.html">Mr. Zhang Ziqi</a> — PhD Student, End-to-End Learning &amp; Sensor Fusion<br />
<a href="https://polyu-taslab.github.io/members/Huang_Feng.html">Dr. Huang Feng</a> — Postdoctoral Researcher, Navigation &amp; Localization</p>

<hr />

<p><strong>Research Focus:</strong> End-to-End Deep Learning, Vision-Language Navigation, Multi-Modal Sensor Fusion, Sim-to-Real Transfer, Safe Reinforcement Learning, Imitation Learning, World Models for Autonomous Driving</p>]]></content><author><name></name></author><category term="Autonomous-Driving" /><summary type="html"><![CDATA[Our cutting-edge research platforms for end-to-end AI self-driving, where neural networks learn to drive directly from sensor data to control outputs.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/preview/pr-/images/project/Vehicle/ADV.png" /><media:content medium="image" url="/preview/pr-/images/project/Vehicle/ADV.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Reliable UAV Perception and Perching Solutions in Urban Streets</title><link href="/preview/pr-/2024/12/09/Reliable_UAV_Perception_and_Perching_Solutions_in_Urban_Streets.html" rel="alternate" type="text/html" title="Reliable UAV Perception and Perching Solutions in Urban Streets" /><published>2024-12-09T00:00:00+00:00</published><updated>2026-02-15T05:28:27+00:00</updated><id>/preview/pr-/2024/12/09/Reliable_UAV_Perception_and_Perching_Solutions_in_Urban_Streets</id><content type="html" xml:base="/preview/pr-/2024/12/09/Reliable_UAV_Perception_and_Perching_Solutions_in_Urban_Streets.html"><![CDATA[<p>Develop a comprehensive UAV perception and Perching solution, focusing on the integration of smart streetlight poles with a UAV takeoff, landing, and battery exchange platform</p>

<h2 id="abstract">Abstract</h2>

<p>This project aims to design and implement a comprehensive UAV perching and management system, tailored to support
autonomous operations in diverse environments. Central to the system is the development of UAV airports equipped
with wireless charging modules, April Tag-based precise landing platforms, and integrated position and orientation
correction mechanisms. These features enable reliable takeoff, landing, and continuous UAV operation with minimal
human intervention.</p>

<p>The system incorporates a robust power management framework, including a custom-designed power distribution board
for seamless transition between onboard battery and external power during charging. Communication between UAVs
and the airport leverages 4G MQTT protocols, ensuring reliable real-time data exchange. Advanced algorithms
integrate GNSS, Vicon, and April Tag-based localization for high-precision positioning, further enhanced by extended
Kalman filtering for improved dynamic performance.</p>

<p>Rigorous experimental validation confirms the system’s effectiveness, including tests on April Tag-based localization,
power management, and wireless communication under dynamic conditions. This project not only addresses critical
challenges in UAV operation and management but also lays the groundwork for scalable UAV infrastructure, fostering
innovation and efficiency in both urban and remote applications.</p>

<div style="display: flex; justify-content: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/UAV_Perching/Smart_Street_light_Poles_with_UAV_Airports.png" alt="Team Banner" style="width: 60%; height: auto; object-fit: cover; max-width: 500px; margin: 0 10px; border-radius: 15px;" />
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/UAV_Perching/Perching_UAV_Flight.jpg" alt="Perching UAV Flight" style="width: 70%; height: auto; object-fit: cover; max-width: 500px; margin: 0 10px; border-radius: 15px;" />
</div>

<h2 id="funding-body">Funding Body</h2>

<p>POLYU AAE(Capstone Project)</p>

<h2 id="researcher">Researcher</h2>

<p><a href="https://polyu-taslab.github.io/members/Wen_Weisong.html">Dr. Weisong Wen</a>, <a href="https://polyu-taslab.github.io/members/ZHAO_Jiaqi.html">Mr. ZHAO Jiaqi</a>, Mr. Li Mingjue, <a href="https://polyu-taslab.github.io/members/hujiahao.html">Mr. HU Jiahao</a>, <a href="https://polyu-taslab.github.io/members/Xiao_Naigui.html">Mr. Xiao Naigui</a>, Mr. FU Chenlei, Mr. Yangmokui</p>

<h2 id="status">Status</h2>

<div style="display: flex; justify-content: center; gap: 20px; margin-bottom: 20px;">
  <div style="text-align: center; margin: 0 10px; display: flex; flex-direction: column; align-items: center;">
    <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/UAV_Perching/compressed_landing_successful.gif" alt="Team Banner" style="width: 60%; height: auto; object-fit: cover; max-width: 600px; border-radius: 8px;" />
    <p style="margin-top: 10px; text-align: center;">UAV Landing Demo</p>
  </div>
  <div style="text-align: center; margin: 0 10px; display: flex; flex-direction: column; align-items: center;">
    <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/UAV_Perching/UAV_Airport.png" alt="Team Banner" style="width: 110%; height: auto; object-fit: cover; max-width: 600px; border-radius: 8px;" />
    <p style="margin-top: 10px; text-align: center;">UAV Airport</p>
  </div>
  <div style="text-align: center; margin: 0 10px; display: flex; flex-direction: column; align-items: center;">
    <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/UAV_Perching/Power_Distribution_Board.png" alt="Team Banner" style="width: 93%; height: auto; object-fit: cover; max-width: 600px; border-radius: 8px;" />
    <p style="margin-top: 10px; text-align: center;">UAV Battery Wireless Charging and Health<br />Management Solution</p>
  </div>
</div>]]></content><author><name></name></author><category term="Landing," /><category term="UAV" /><category term="Perching," /><category term="UAV" /><category term="Airport" /><summary type="html"><![CDATA[Develop a comprehensive UAV perception and Perching solution, focusing on the integration of smart streetlight poles with a UAV takeoff, landing, and battery exchange platform]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/preview/pr-/images/project/UAV_Perching/Smart_Street_light_Poles_with_UAV_Airports.png" /><media:content medium="image" url="/preview/pr-/images/project/UAV_Perching/Smart_Street_light_Poles_with_UAV_Airports.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">AI assisted inertial navigation system</title><link href="/preview/pr-/2024/10/14/AI_assisted_inertial_navigation_system.html" rel="alternate" type="text/html" title="AI assisted inertial navigation system" /><published>2024-10-14T00:00:00+00:00</published><updated>2026-02-15T05:28:27+00:00</updated><id>/preview/pr-/2024/10/14/AI_assisted_inertial_navigation_system</id><content type="html" xml:base="/preview/pr-/2024/10/14/AI_assisted_inertial_navigation_system.html"><![CDATA[<p>This project aims to develop a deep learning-based inertial navigation algorithm that utilizes accelerometer, gyroscope, and magnetometer data from smart wearables and smartphones to infer the user’s position and movement trajectory, while providing corresponding confidence levels</p>

<h2 id="introduction">Introduction</h2>
<p>Inertial odometry is a critical technology used in various applications, from robotics and autonomous vehicles to augmented reality (AR) and wearable devices. It involves estimating the position and orientation of an object over time using data from inertial measurement units (IMUs), which typically include accelerometers and gyroscopes. However, traditional inertial odometry systems often face challenges such as sensor noise, bias, and drift, which can lead to cumulative errors and reduced accuracy over time. To address these challenges, AI-aided inertial odometry has emerged as a promising solution, leveraging the power of artificial intelligence to enhance the performance and reliability of inertial navigation systems. By integrating AI techniques such as machine learning and sensor fusion, these systems can intelligently process and interpret IMU data, correcting for errors and improving overall accuracy. AI-aided inertial odometry systems can learn from patterns in sensor data, adapt to different environments, and integrate information from multiple sources, such as cameras and GPS, to provide more robust and precise motion tracking. This advancement not only mitigates the limitations of traditional inertial systems but also opens up new possibilities for applications in complex and dynamic environments where traditional methods may fall short. As AI continues to evolve, its integration with inertial odometry is expected to drive significant innovations across various fields, enhancing the capabilities of autonomous systems and enriching user experiences in wearable devices. <strong>This project aims to develop a deep learning-based inertial navigation algorithm that utilizes accelerometer, gyroscope, and magnetometer data from smart wearables and smartphones to infer the user’s position and movement trajectory, while providing corresponding confidence levels</strong>.</p>

<h2 id="funding-body">Funding Body</h2>
<p>Honor 上海榮耀智慧科技開發有限公司 (Collaborative Research)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/Honorr.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="Inertial" /><category term="Navigation" /><category term="System," /><category term="AI" /><summary type="html"><![CDATA[This project aims to develop a deep learning-based inertial navigation algorithm that utilizes accelerometer, gyroscope, and magnetometer data from smart wearables and smartphones to infer the user’s position and movement trajectory, while providing corresponding confidence levels]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/preview/pr-/images/project/Honor.png" /><media:content medium="image" url="/preview/pr-/images/project/Honor.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Development of an Assisted Navigation and Collision Avoidance System using AI and Location-based Service</title><link href="/preview/pr-/2024/04/08/Development_of_an_Assisted_Navigation_and_Collision_Avoidance_System_using_AI_and_Location-based_Service.html" rel="alternate" type="text/html" title="Development of an Assisted Navigation and Collision Avoidance System using AI and Location-based Service" /><published>2024-04-08T00:00:00+00:00</published><updated>2026-02-15T05:28:27+00:00</updated><id>/preview/pr-/2024/04/08/Development_of_an_Assisted_Navigation_and_Collision_Avoidance_System_using_AI_and_Location-based_Service</id><content type="html" xml:base="/preview/pr-/2024/04/08/Development_of_an_Assisted_Navigation_and_Collision_Avoidance_System_using_AI_and_Location-based_Service.html"><![CDATA[<h2 id="abstract">Abstract</h2>

<p>Urban roadways often face challenges such as building-induced obstructions, creating significant blind spots and positioning inaccuracies that hinder safe lane spacing and inter-vehicle distance management. Existing solutions lack cost-effective enhancements for positioning and over-the-horizon collaborative sensing technologies, particularly in dense urban environments.</p>

<p>To address these issues, this project proposes a low-cost, high-precision co-location solution specifically designed for urban canyons. Additionally, a collision avoidance warning application is developed to provide early warnings and emergency interventions for collision risks in blind spots.</p>

<p>The proposed approach integrates advanced methodologies: (1) Lane-level positioning services utilizing Real-Time Kinematic (RTK) and 3D city maps for precise lane-level positioning, (2) Multi-vehicle collaborative sensing leveraging artificial intelligence (AI) and RTK to enable over-the-horizon sensing and early warnings, and (3) Dynamic model optimization using AI to refine positioning accuracy and develop a prototype multi-vehicle collision avoidance system.</p>

<p>The solution achieves lane-level positioning accuracy with an error margin of less than 0.5 meters and provides collision risk warnings at varying inter-vehicle distances. By significantly reducing traffic accidents caused by urban road challenges, this project offers a transformative step toward safer and more reliable urban transportation systems.</p>

<h2 id="funding-body">Funding Body</h2>

<p>Smart Traffic Fund</p>

<h2 id="researcher">Researcher</h2>

<p><a href="https://polyu-taslab.github.io/members/Wen_Weisong.html">Dr. Weisong Wen</a>, Dr. Li-Ta Hsu, Dr. Guohao Zhang, Dr. Jian Liu, <a href="https://polyu-taslab.github.io/members/Huang_Feng.html">Feng Huang</a>, <a href="https://polyu-taslab.github.io/members/liu_xikun.html">Xikun Liu</a>， <a href="https://polyu-taslab.github.io/members/yihan_zhong.html">Yihan Zhong</a>， <a href="https://polyu-taslab.github.io/members/wang_xin.html">Xin Wang</a>, Yuan Li， <a href="https://polyu-taslab.github.io/members/runzhi_hu.html">Runzhi Hu</a></p>

<h2 id="status">Status</h2>

<p>Completed</p>

<h2 id="system-framework">System Framework</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/stf/pipeline.png" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="demonstration">Demonstration</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/stf/demo_gif.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="achievements">Achievements</h2>

<ul>
  <li>Li, Y., Liu, X., Wen, W., Hsu, L. T., Yuan, Y., Bian, G., &amp; Chen, Q. (2024, September). Factor Graph Optimization Based Multi Epoch Ambiguity Resolution for GNSS RTK and its Evaluation in Hong Kong Urban Canyons. In Proceedings of the 37th International Technical Meeting of the Satellite Division of The Institute of Navigation (ION GNSS+ 2024) (pp. 2151-2162).</li>
</ul>]]></content><author><name></name></author><category term="Positioning" /><category term="Services," /><category term="Multi-Vehicle" /><category term="Collaborative" /><category term="Sensing," /><category term="AI" /><category term="aided" /><category term="GNSS," /><category term="GNSS" /><category term="Signal" /><category term="Tracing," /><category term="Sensor" /><category term="Integration" /><summary type="html"><![CDATA[Abstract]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/preview/pr-/images/project/stf/demo_gif.gif" /><media:content medium="image" url="/preview/pr-/images/project/stf/demo_gif.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Safe-assured Learning-based Deep SE(3) Motion Joint Planning and Control for Unmanned Aerial Vehicles</title><link href="/preview/pr-/2024/04/08/Safe-assured_Learning-based_Deep_SE(3)_Motion_Joint_Planning_and_Control_for_Unmanned_Aerial_Vehicles.html" rel="alternate" type="text/html" title="Safe-assured Learning-based Deep SE(3) Motion Joint Planning and Control for Unmanned Aerial Vehicles" /><published>2024-04-08T00:00:00+00:00</published><updated>2026-02-15T05:28:27+00:00</updated><id>/preview/pr-/2024/04/08/Safe-assured_Learning-based_Deep_SE(3)_Motion_Joint_Planning_and_Control_for_Unmanned_Aerial_Vehicles</id><content type="html" xml:base="/preview/pr-/2024/04/08/Safe-assured_Learning-based_Deep_SE(3)_Motion_Joint_Planning_and_Control_for_Unmanned_Aerial_Vehicles.html"><![CDATA[<p>PolyU (UGC)</p>

<h2 id="funding-body">Funding Body</h2>

<p>PolyU (UGC)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="Advanced" /><category term="Vehicle" /><category term="Safety" /><category term="Systems," /><category term="Automated" /><category term="Vehicle" /><category term="Operation," /><category term="Motion" /><category term="Planning," /><category term="Navigation," /><category term="Aerial," /><category term="Marine" /><category term="and" /><category term="Surface" /><category term="Intelligent" /><category term="Vehicles" /><summary type="html"><![CDATA[PolyU (UGC)]]></summary></entry><entry><title type="html">Sustainable Window Cleaning for PolyU Jockey Club Innovation Tower with Unmanned Aerial Vehicles (UAV):An Application of Autonomous Systems Enabled Carbon Reduction</title><link href="/preview/pr-/2024/04/01/Sustainable_Window_Cleaning_for_PolyU_Jockey_Club_Innovation_Tower_with_Unmanned_Aerial_Vehicles-(UAV)_An_Application_of_Autonomous_Systems_Enabled_Carbon_Reduction.html" rel="alternate" type="text/html" title="Sustainable Window Cleaning for PolyU Jockey Club Innovation Tower with Unmanned Aerial Vehicles (UAV):An Application of Autonomous Systems Enabled Carbon Reduction" /><published>2024-04-01T00:00:00+00:00</published><updated>2026-02-15T05:28:27+00:00</updated><id>/preview/pr-/2024/04/01/Sustainable_Window_Cleaning_for_PolyU_Jockey_Club_Innovation_Tower_with_Unmanned_Aerial_Vehicles%20(UAV)_An_Application_of_Autonomous_Systems_Enabled_Carbon_Reduction</id><content type="html" xml:base="/preview/pr-/2024/04/01/Sustainable_Window_Cleaning_for_PolyU_Jockey_Club_Innovation_Tower_with_Unmanned_Aerial_Vehicles-(UAV)_An_Application_of_Autonomous_Systems_Enabled_Carbon_Reduction.html"><![CDATA[<p>PolyU (UGC)</p>

<h2 id="introduction">Introduction</h2>
<p>Skyscrapers are one of the features of Hong Kong and its window cleaning is a time-consuming and dangerous work that needs to be completed regularly. The typical solution is to use big machines, such as the “Movable platform” or the “Bosun’s chair” with high power, to navigate the window cleaners to the building’s exterior. Take the PolyU Jockey Club Innovation Tower (JCIT) as an example, 20,000 HKD are required by the cleaning industry to clean 100 m^2 areas (less than 1/20 of the total areas of the JCIT surface) in two weeks. Based on the fuel assumption, a value of carbon emission of 277.8 kgCO2 was caused by the JCIT cleaning in two weeks. The value of the carbon emission will be even more significant when cleaning the rest of the JCIT due to the irregular shape of the JCIT. In fact, part of the JCIT surface is very difficult to clean due to the highly irregular surface which is out of the reach of the typical “Movable platform” or the “Bosun’s chair”. Bigger machines are required but lead to unacceptable carbon reduction accordingly. A solution to efficiently complete the window cleaning is practical and highly expected, which can directly help with carbon reduction at the PolyU campus. Moreover, this is highly expected by lots of “Skyscrapers” in Hong Kong.</p>

<p>To fill this gap, this project proposes to use unmanned aerial vehicles (UAV) to complete the building window cleaning task efficiently after several meeting discussions between the project team members and the PolyU Campus Facilities and Sustainability Office (CFSO), aiming at reducing the carbon emission from existing solutions. The project team has previously accumulated significant achievements in terms of UAV technologies, such as navigation, control, and planning, which is supported by the team’s academic publications records and knowledge transfer to the UAV industry, such as the Meituan with applications for parcel delivery. Also, the project team has the capability of designing the UAV system from scratch. Based on the prototype of the UAV for building window cleaning, the value of carbon emission would be reduced from 277.8 kgCO2 using the existing solution by the cleaning industry, to around 2.91 kgCO2 using the UAV system. The achievement in carbon reduction can be even more significant upon the success and completion of this project. We fully believe that this project will introduce significant impacts to the carbon reduction plan of PolyU, and will also lead to potential impacts to the carbon reduction of Hong Kong.</p>

<h2 id="funding-body">Funding Body</h2>
<p>Carbon Neutrality Funding Scheme 2023/24 (PolyU)</p>

<h2 id="researcher">Researcher</h2>
<p><a href="https://polyu-taslab.github.io/members/Wen_Weisong.html">Dr. Weisong Wen</a></p>

<h2 id="status">Status</h2>
<p>Ongoing</p>

<h2 id="technology--method">Technology &amp; Method</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/green_tech.png" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="potential-contributions-to-carbon-reduction">Potential Contributions to Carbon Reduction</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/carbon_reduce.png" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="uav-test">UAV Test</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <h1 style="font-size: 1.5em; margin-bottom: 20px; line-height: 1.2;">
    Outdoor test
  </h1>
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/windows_cleaning_test.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<div style="text-align: center; margin-bottom: 20px;">
  <h1 style="font-size: 1.5em; margin-bottom: 20px; line-height: 1.2;">
    Indoor test
  </h1>
  <iframe width="560" height="315" src="https://www.youtube.com/embed/m2Lm8RY2uYI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</div>

<h2 id="interview">Interview</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <h1 style="font-size: 1.5em; margin-bottom: 20px; line-height: 1.2;">
    TVB interviewed us about the low-altitude economy (LAE)
  </h1>
  <img width="560" height="315" src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/TVB.JPG" alt="TVB Interview about LAE" />
  <iframe width="560" height="315" src="//player.bilibili.com/player.html?bvid=BV1bhrKY3EcJ&amp;spm_id_from=333.999.0.0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe>
</div>

<p style="text-align: left; font-size: 1.0em; margin-bottom: 10px;">
  Video link:
</p>
<div style="text-align: left; font-size: 1.0em;">
  <a href="https://evtolasia.com/2024/06/01/hong-kongs-low-altitude-economy-opportunities-and-policy-considerations" target="_blank" style="display: block; color: #007BFF; text-decoration: none;">
    https://evtolasia.com/2024/06/01/hong-kongs-low-altitude-economy-opportunities-and-policy-considerations
  </a>
  <a href="https://news.tvb.com/tc/programme/financemagazine" target="_blank" style="display: block; color: #007BFF; text-decoration: none;">
    https://news.tvb.com/tc/programme/financemagazine
  </a>
  <a href="https://news.tvb.com/tc/finance/665a72c4b08fa1987b3d6350/財經-財經透視香港低空經濟何時起飛" target="_blank" style="display: block; color: #007BFF; text-decoration: none;">
    https://news.tvb.com/tc/finance/665a72c4b08fa1987b3d6350/財經-財經透視香港低空經濟何時起飛
  </a>
</div>

<div style="text-align: center; margin-bottom: 20px;">
  <h1 style="font-size: 1.5em; margin-bottom: 20px; line-height: 1.2;">
    Mingpao interviewed us about the low-altitude economy (LAE)
  </h1>
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/MINGBAO.JPG" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<p style="text-align: left; font-size: 1.0em; margin-bottom: 10px;">
  Video link:
</p>
<div style="text-align: left; font-size: 1.0em;">
  <a href="https://news.mingpao.com/pns/港聞/article/20240531/s00002/1717094795920/學者倡訂法規認證基建助低空經濟-研無人機洗大廈外牆-料潛在利潤每年逾3億" target="_blank" style="display: block; color: #007BFF; text-decoration: none;">
    https://news.mingpao.com/pns/港聞/article/20240531/s00002/1717094795920/學者倡訂法規認證基建助低空經濟-研無人機洗大廈外牆-料潛在利潤每年逾3億
  </a>
  <a href="https://www.youtube.com/watch?v=m2Lm8RY2uYI" target="_blank" style="display: block; color: #007BFF; text-decoration: none;">
    https://www.youtube.com/watch?v=m2Lm8RY2uYI
  </a>
</div>

<div style="text-align: center; margin-bottom: 20px;">
  <h1 style="font-size: 1.5em; margin-bottom: 20px; line-height: 1.2;">
    Interview with RTHK on the Development of Drones in the Low Altitude Economy (LAE)
  </h1>
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/rthk.jpg" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<p style="text-align: left; font-size: 1.0em; margin-bottom: 10px;">
  Video link:
</p>
<div style="text-align: left; font-size: 1.0em;">
  <a href="https://www.rthk.hk/tv/dtt32/programme/vibranthongkong/episode/959611" target="_blank" style="display: block; color: #007BFF; text-decoration: none;">
    https://www.rthk.hk/tv/dtt32/programme/vibranthongkong/episode/959611
  </a>
</div>]]></content><author><name></name></author><category term="Unmanned" /><category term="Aerial" /><category term="Vehicle" /><summary type="html"><![CDATA[PolyU (UGC)]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/preview/pr-/images/project/uav_clean.png" /><media:content medium="image" url="/preview/pr-/images/project/uav_clean.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Data-driven-assisted GNSS RTK/INS Navigation for Autonomous Systems in Urban Canyons </title><link href="/preview/pr-/2024/01/01/Data-driven-assisted_GNSS_RTK-INS_Navigation_for_Autonomous_Systems_in_Urban_Canyons.html" rel="alternate" type="text/html" title="Data-driven-assisted GNSS RTK/INS Navigation for Autonomous Systems in Urban Canyons " /><published>2024-01-01T00:00:00+00:00</published><updated>2026-02-15T05:28:27+00:00</updated><id>/preview/pr-/2024/01/01/Data-driven-assisted_GNSS_RTK-INS_Navigation_for_Autonomous_Systems_in_Urban_Canyons</id><content type="html" xml:base="/preview/pr-/2024/01/01/Data-driven-assisted_GNSS_RTK-INS_Navigation_for_Autonomous_Systems_in_Urban_Canyons.html"><![CDATA[<h2 id="abstract">Abstract</h2>

<p>The great development of smart urban transportation has led to the booming of the autonomous vehicle industry. Society places great expectations on autonomous driving to enhance safety and efficiency in transportation systems, particularly in challenging environments like urban canyons where precise positioning is crucial. Global Navigation Satellite System (GNSS) positioning modules are the crucial element of autonomous vehicles for absolute positioning. However, the GNSS pseudorange suffers significant biases and degradation due to the multipath and non-line-of-sight (NLOS) errors caused by tall buildings that widely exist in urban canyons. To improve GNSS positioning accuracy in harsh urban canyons, this paper introduces a deep multimodal learning-based approach to correct distorted pseudorange measurements. The vision features of the environment captured by the camera are integrated to form the multimodal network with GNSS measurements. Meanwhile, positional encoding (PE) is proposed to fuse the image features and the satellite position information in higher-dimensional space. The biases are outputted by the network and are used for improved positioning. Compared to existing solutions, extensive test results show that the proposed method achieves improved positioning accuracy ranging from 12\% to 20\%. Additionally, the method proposed in this paper takes only 0.5 seconds to perform epoch-by-epoch pseudorange correction on Jetson Orin Nano, which meets the real-time requirements and shows extremely high practical value. This paper also opensources a dataset under varying time, satellite distributions, and lighting conditions with accurate NLOS labels. We aim to contribute this dataset to the GNSS and deep learning communities, aspiring to establish it as the benchmark for NLOS classification and pseudorange bias correction. The datasets can be accessed at <a href="https://github.com/ebhrz/KLTDataset">github</a>.</p>

<h2 id="funding-body">Funding Body</h2>

<p>PolyU (UGC)</p>

<h2 id="researcher">Researcher</h2>

<p><a href="https://polyu-taslab.github.io/members/Wen_Weisong.html">Dr. Weisong Wen</a>, <a href="https://polyu-taslab.github.io/members/runzhi_hu.html">Runzhi Hu</a>, Dr. Jian Liu, <a href="https://polyu-taslab.github.io/members/yihan_zhong.html">Yihan Zhong</a>, Dr. Ming Xia</p>

<h2 id="status">Status</h2>

<p>Undergoing</p>

<h2 id="system-framework">System Framework</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/Data_driven_structure.png" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="positioning-results">Positioning Results</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/Data_driven_boxplot.png" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="achievements">Achievements</h2>

<ul>
  <li>Dataset Link: <a href="https://github.com/ebhrz/KLTDataset">https://github.com/ebhrz/KLTDataset</a></li>
</ul>]]></content><author><name></name></author><category term="GNSS" /><category term="Positioning," /><category term="NLOS/Multipath" /><category term="Correction," /><category term="Autonomous" /><category term="Driving," /><category term="Positional" /><category term="Encoding," /><category term="Multimodal" /><category term="Network," /><category term="Vision" /><category term="Feature" /><summary type="html"><![CDATA[Abstract]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/preview/pr-/images/project/Data_driven_structure.png" /><media:content medium="image" url="/preview/pr-/images/project/Data_driven_structure.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Maximum Consensus Integration of GNSS and LiDAR for Urban Navigation</title><link href="/preview/pr-/2024/01/01/Maximum_Consensus_Integration_of_GNSS_and_LiDAR_for_Urban_Navigation.html" rel="alternate" type="text/html" title="Maximum Consensus Integration of GNSS and LiDAR for Urban Navigation" /><published>2024-01-01T00:00:00+00:00</published><updated>2026-02-15T05:28:27+00:00</updated><id>/preview/pr-/2024/01/01/Maximum_Consensus_Integration_of_GNSS_and_LiDAR_for_Urban_Navigation</id><content type="html" xml:base="/preview/pr-/2024/01/01/Maximum_Consensus_Integration_of_GNSS_and_LiDAR_for_Urban_Navigation.html"><![CDATA[<p>PolyU (UGC)</p>

<h2 id="funding-body">Funding Body</h2>

<p>PolyU (UGC)</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/huawei_mapping.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>]]></content><author><name></name></author><category term="GNSS," /><category term="LIDAR," /><category term="Sensor" /><category term="fusion" /><summary type="html"><![CDATA[PolyU (UGC)]]></summary></entry><entry><title type="html">Multi-robot Collaborative Operations in Lunar Areas for Regolith Processing</title><link href="/preview/pr-/2023/12/03/Multi_robot_Collaborative_Operations_in_Lunar_Areas_for_Regolith_Processing_Project.html" rel="alternate" type="text/html" title="Multi-robot Collaborative Operations in Lunar Areas for Regolith Processing" /><published>2023-12-03T00:00:00+00:00</published><updated>2026-02-15T05:28:27+00:00</updated><id>/preview/pr-/2023/12/03/Multi_robot_Collaborative_Operations_in_Lunar_Areas_for_Regolith_Processing_Project</id><content type="html" xml:base="/preview/pr-/2023/12/03/Multi_robot_Collaborative_Operations_in_Lunar_Areas_for_Regolith_Processing_Project.html"><![CDATA[<h2 id="abstract">Abstract</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/rugged_surface_problem_lunar.jpg" alt="Team Banner" style="width: 50%; height: auto; object-fit: cover; max-width: 350px; margin: 0 auto; border-radius: 15px;" />
</div>

<p>Research conducted on the lunar surface necessitates the utilization of Unmanned Autonomous Systems (UASs) equipped with sophisticated robotic arms designed for the purpose of collecting soil samples. The primary challenges encountered in this endeavor revolve around the need for precise control over the UASs while navigating through rugged and uneven terrain, all while ensuring the utmost safety of the operation. To tackle these challenges head-on, a cutting-edge multi-robot collaboration system has been conceptualized and put forth as a solution. This innovative system leverages the power of Factor Graph Optimization (FGO) to achieve unparalleled accuracy in positioning the UASs during the soil sample collection process.</p>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/positioning error.jpg" alt="Team Banner" style="width: 50%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<p>Remarkably, the implementation of this collaborative system has yielded remarkable results, boasting a Root Mean Square Error (RMSE) of merely 0.30 meters. This achievement represents a substantial advancement compared to the conventional methods previously employed in similar research endeavors. A pivotal component of this system is the integration of the Leader-Follower Formation Algorithm, which plays a crucial role in facilitating efficient coordination among multiple robots operating simultaneously in the lunar environment.</p>

<p>Looking ahead, the future research trajectory will be centered around further enhancing the collaborative capabilities of the system. This will entail a deep dive into refining the existing Positioning and Leader-Follower algorithms, with a specific focus on optimizing autonomous navigation techniques and streamlining the soil sampling process. Noteworthy features of the system include automated navigation functionalities, advanced soil identification mechanisms, and seamless soil collection procedures facilitated by the utilization of QR codes for precise localization and task allocation purposes.</p>

<h2 id="funding-body">Funding Body</h2>

<p>Research Centre for Deep Space Explorations  (Dec 2023 - Dec 2025)</p>

<h2 id="researcher">Researcher</h2>

<p><a href="https://polyu-taslab.github.io/members/Wen_Weisong.html">Dr. Weisong Wen</a>, Qian Liang</p>

<h2 id="status">Status</h2>

<p>Ongoing</p>

<h2 id="system-framework">System Framework</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/prototype.png" alt="Team Banner" style="width: 60%; height: auto; object-fit: cover; max-width: 320px; margin: 0 auto; border-radius: 15px;" />
</div>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/overall_architecture_lunar.jpg" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="mapping-results">Mapping Results</h2>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/speed_4_combined_arms.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<div style="text-align: center; margin-bottom: 20px;">
  <img src="https://github.com/PolyU-TASLAB/polyu-taslab.github.io/raw/main/images/project/speedy_clipped_ms_AO.gif" alt="Team Banner" style="width: 100%; height: auto; object-fit: cover; max-width: 850px; margin: 0 auto; border-radius: 15px;" />
</div>

<h2 id="achievements">Achievements</h2>]]></content><author><name></name></author><category term="Multi-robot-collaboration," /><category term="MPC," /><category term="mapping," /><category term="Leader-Follower-Formation-Algorithm," /><category term="sensor-fusion," /><category term="LiDAR," /><category term="IMU" /><summary type="html"><![CDATA[Abstract]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/preview/pr-/images/project/prototype.png" /><media:content medium="image" url="/preview/pr-/images/project/prototype.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>